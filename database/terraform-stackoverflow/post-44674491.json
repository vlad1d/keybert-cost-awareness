{"Id": "44674491", "PostTypeId": "1", "AcceptedAnswerId": "44687215", "CreationDate": "2017-06-21T11:12:40.707", "Score": "5", "ViewCount": "2391", "Body": "<p>Terraform 0.9.5. </p>\n\n<p>I am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  </p>\n\n<p>Like all teams using <code>terraform</code> shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... </p>\n\n<p>We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  </p>\n\n<p>I authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  </p>\n\n<p>Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)</p>\n\n<p><strong>Update</strong>:Got this working.  I created a new user within the account where the s3 bucket is created.  Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table.  Created a custom credentials file and added default profile with access and secret keys assigned to that new user.  Used the backend config similar to </p>\n\n<pre><code>terraform {\n   required_version = \"&gt;= 0.9.5\"\n\n   backend \"s3\" {\n      bucket                  = \"remote_state\"\n      key                     = \"/NAME_OF_STACK/terraform.tfstate\"\n      region                  = \"us-east-1\"\n      encrypt                 = \"true\"\n      shared_credentials_file = \"PATH_TO_CUSTOM_CREDENTAILS_FILE\"\n      lock_table              = \"MY_LOCK_TABLE\"\n    }\n}\n</code></pre>\n\n<p>It works but there is an initial configuration that needs to happen within your profile to get it working.  If anybody knows of a better setup or can identify problems with my backend config please let me know.</p>\n", "OwnerUserId": "328013", "LastEditorUserId": "1466687", "LastEditDate": "2019-08-06T01:19:24.630", "LastActivityDate": "2019-08-06T01:19:24.630", "Title": "Terraform Shared State", "Tags": "<amazon-s3><amazon-dynamodb><terraform>", "AnswerCount": "1", "CommentCount": "3", "FavoriteCount": "0", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "76357888", "PostId": "44674491", "Score": "0", "Text": "From your description, a lot of primarily opinions are based.\n\nTerraform never asks you to share state file.", "CreationDate": "2017-06-21T21:58:59.263", "UserId": "498256", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "76418691", "PostId": "44674491", "Score": "0", "Text": "It seems to be working as expected. I use a different key for each stack and I use the one dynamodb table.", "CreationDate": "2017-06-23T09:47:49.307", "UserId": "328013", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "90692048", "PostId": "44674491", "Score": "0", "Text": "@BMW this is recommended. \"When working with Terraform in a team, use of a local file makes Terraform usage complicated because each user must make sure they always have the latest state data before running Terraform and make sure that nobody else runs Terraform at the same time\" https://www.terraform.io/docs/state/remote.html", "CreationDate": "2018-08-16T05:48:24.707", "UserId": "516748", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "149868816", "PostHistoryTypeId": "2", "PostId": "44674491", "RevisionGUID": "60b89728-4067-4373-a836-68397a6b1ca6", "CreationDate": "2017-06-21T11:12:40.707", "UserId": "328013", "Text": "Terraform 0.9.5. I am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  Like all teams using terraform shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  I authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Text", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}]}, {"Id": "149868817", "PostHistoryTypeId": "1", "PostId": "44674491", "RevisionGUID": "60b89728-4067-4373-a836-68397a6b1ca6", "CreationDate": "2017-06-21T11:12:40.707", "UserId": "328013", "Text": "Terraform Shared State", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "149868818", "PostHistoryTypeId": "3", "PostId": "44674491", "RevisionGUID": "60b89728-4067-4373-a836-68397a6b1ca6", "CreationDate": "2017-06-21T11:12:40.707", "UserId": "328013", "Text": "<amazon-s3><locking><amazon-dynamodb><state><terraform>", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "149895980", "PostHistoryTypeId": "5", "PostId": "44674491", "RevisionGUID": "87809b93-fc45-4928-84d8-aafbb779d354", "CreationDate": "2017-06-21T16:21:25.103", "UserId": "328013", "Comment": "added 1007 characters in body", "Text": "Terraform 0.9.5. I am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  Like all teams using terraform shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  I authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)\r\n\r\nUpdate: Kinda got this working.  I created a new user within the account where the s3 bucket is created.  Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table.  Created a custom credentials file and added default profile with access and secret keys assigned to that new user.  Used the backend config similar to \r\n\r\n    terraform {\r\n    required_version = \">= 0.9.5\"\r\n\r\n    backend \"s3\" {\r\n    bucket                  = \"remote_state\"\r\n    key                     = \"/terraform.tfstate\"\r\n    region                  = \"us-east-1\"\r\n    encrypt                 = \"true\"\r\n    shared_credentials_file = \"PATH_TO_CUSTOM_CREDENTAILS_FILE\"\r\n    lock_table              = \"MY_LOCK_TABLE\"\r\n    }\r\n    }\r\n\r\nIt works but there is initial configuration that needs to happen within your profile to get it working.  If anybody knows of a better setup or can identify problems with my backend config please let me know.", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Text", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}, {"source": "Text", "text": "Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table. ", "keywords": ["policy"]}]}, {"Id": "149917006", "PostHistoryTypeId": "5", "PostId": "44674491", "RevisionGUID": "5f74f7b8-ad89-4a22-a238-035d305e4b55", "CreationDate": "2017-06-21T21:55:45.753", "UserId": "498256", "Comment": "added 33 characters in body", "Text": "Terraform 0.9.5. \r\n\r\nI am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  \r\n\r\nLike all teams using `terraform` shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... \r\n\r\nWe have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  \r\n\r\nI authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  \r\n\r\nIs there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)\r\n\r\n**Update**: Kinda got this working.  I created a new user within the account where the s3 bucket is created.  Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table.  Created a custom credentials file and added default profile with access and secret keys assigned to that new user.  Used the backend config similar to \r\n\r\n    terraform {\r\n    required_version = \">= 0.9.5\"\r\n    \r\n    backend \"s3\" {\r\n    bucket                  = \"remote_state\"\r\n    key                     = \"/terraform.tfstate\"\r\n    region                  = \"us-east-1\"\r\n    encrypt                 = \"true\"\r\n    shared_credentials_file = \"PATH_TO_CUSTOM_CREDENTAILS_FILE\"\r\n    lock_table              = \"MY_LOCK_TABLE\"\r\n    }\r\n    }\r\n\r\nIt works but there is an initial configuration that needs to happen within your profile to get it working.  If anybody knows of a better setup or can identify problems with my backend config please let me know.", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Text", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}, {"source": "Text", "text": "Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table. ", "keywords": ["policy"]}]}, {"Id": "149920928", "PostHistoryTypeId": "6", "PostId": "44674491", "RevisionGUID": "e97a50eb-1e0e-4f41-a829-1f30db27e2e9", "CreationDate": "2017-06-21T23:44:23.937", "UserId": "498256", "Comment": "edited tags", "Text": "<amazon-s3><amazon-dynamodb><terraform>", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": []}, {"Id": "150040576", "PostHistoryTypeId": "5", "PostId": "44674491", "RevisionGUID": "7bdfd66f-ceed-4cdb-86a7-a3084b1e6833", "CreationDate": "2017-06-23T09:52:35.600", "UserId": "328013", "Comment": "added 7 characters in body", "Text": "Terraform 0.9.5. \r\n\r\nI am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  \r\n\r\nLike all teams using `terraform` shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... \r\n\r\nWe have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  \r\n\r\nI authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  \r\n\r\nIs there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)\r\n\r\n**Update**:Got this working.  I created a new user within the account where the s3 bucket is created.  Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table.  Created a custom credentials file and added default profile with access and secret keys assigned to that new user.  Used the backend config similar to \r\n\r\n    terraform {\r\n    required_version = \">= 0.9.5\"\r\n    \r\n    backend \"s3\" {\r\n    bucket                  = \"remote_state\"\r\n    key                     = \"/NAME_OF_STACK/terraform.tfstate\"\r\n    region                  = \"us-east-1\"\r\n    encrypt                 = \"true\"\r\n    shared_credentials_file = \"PATH_TO_CUSTOM_CREDENTAILS_FILE\"\r\n    lock_table              = \"MY_LOCK_TABLE\"\r\n    }\r\n    }\r\n\r\nIt works but there is an initial configuration that needs to happen within your profile to get it working.  If anybody knows of a better setup or can identify problems with my backend config please let me know.", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Text", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}, {"source": "Text", "text": "Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table. ", "keywords": ["policy"]}]}, {"Id": "202937780", "PostHistoryTypeId": "5", "PostId": "44674491", "RevisionGUID": "36a4844d-3ef4-4ee0-b25f-668b411a358d", "CreationDate": "2019-08-06T01:19:24.630", "UserId": "1466687", "Comment": "Formatted code snippet for easier readability", "Text": "Terraform 0.9.5. \r\n\r\nI am in the process of putting together a group of modules that our infrastructure team and automation team will use to create resources in a standard fashion and in turn create stacks to provision different envs. All working well.  \r\n\r\nLike all teams using `terraform` shared state becomes a concern.  I have configured terraform to use a s3 backend, that is versioned and encrypted, added a lock via a dynamo db table. Perfect.  All works with local accounts...  Okay the problem... \r\n\r\nWe have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. My problem is as follows.  \r\n\r\nI authenticate as user in our IAM account and assume the required role.  This has been working like a dream until i introduced terraform backend configuration to utilise s3 for shared state.  It looks like the backend config within terraform requires default credentials to be set within ~/.aws/credentials.  It also looks like these have to be a user that is local to the account where the s3 bucket was created.  \r\n\r\nIs there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider?  Is there a better way to configured shared state and locking?  Any suggestions welcome :)\r\n\r\n**Update**:Got this working.  I created a new user within the account where the s3 bucket is created.  Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table.  Created a custom credentials file and added default profile with access and secret keys assigned to that new user.  Used the backend config similar to \r\n\r\n    terraform {\r\n       required_version = \">= 0.9.5\"\r\n    \r\n       backend \"s3\" {\r\n          bucket                  = \"remote_state\"\r\n          key                     = \"/NAME_OF_STACK/terraform.tfstate\"\r\n          region                  = \"us-east-1\"\r\n          encrypt                 = \"true\"\r\n          shared_credentials_file = \"PATH_TO_CUSTOM_CREDENTAILS_FILE\"\r\n          lock_table              = \"MY_LOCK_TABLE\"\r\n        }\r\n    }\r\n\r\nIt works but there is an initial configuration that needs to happen within your profile to get it working.  If anybody knows of a better setup or can identify problems with my backend config please let me know.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Text", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}, {"source": "Text", "text": "Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table. ", "keywords": ["policy"]}]}, {"Id": "202937781", "PostHistoryTypeId": "24", "PostId": "44674491", "RevisionGUID": "36a4844d-3ef4-4ee0-b25f-668b411a358d", "CreationDate": "2019-08-06T01:19:24.630", "Comment": "Proposed by 1466687 approved by 1723857, 174777 edit id of 4421905", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "answers": [{"Id": "44687215", "PostTypeId": "2", "ParentId": "44674491", "CreationDate": "2017-06-21T22:25:14.840", "Score": "3", "Body": "<p>Terraform expects backend configuration to be static, and does not allow it to include interpolated variables as might be true elsewhere in the config due to the need for the backend to be initialized before any other work can be done.</p>\n\n<p>Due to this, applying the same config multiple times using different AWS accounts can be tricky, but is possible in one of two ways.</p>\n\n<hr>\n\n<p>The lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls.</p>\n\n<p>Organizations adopting this strategy will sometimes create the S3 bucket in a separate \"adminstrative\" AWS account, and then grant restrictive access to the individual state objects in the bucket to the specific roles that will run Terraform in each of the other accounts.</p>\n\n<p>This solution has the advantage that once it has been set up correctly in S3 Terraform can be used routinely without any unusual workflow: configure the single S3 bucket in the backend, and provide appropriate credentials via environment variables to allow them to vary. Once the backend is initialized, use <a href=\"https://www.terraform.io/docs/state/workspaces.html\" rel=\"nofollow noreferrer\">workspaces</a> (known as \"state environments\" prior to Terraform 0.10) to create a separate state for each of the target environments of a single configuration.</p>\n\n<p>The disadvantage is the need to manage a more-complicated access configuration around S3, rather than simply relying on coarse access control with whole AWS accounts. It is also more challenging with DynamoDB in the mix, since the access controls on DynamoDB are not as flexible.</p>\n\n<p>There is a more complete description of this option in the Terraform <code>s3</code> provider documentation, <a href=\"https://www.terraform.io/docs/backends/types/s3.html#multi-account-aws-architecture\" rel=\"nofollow noreferrer\"><em>Multi-account AWS Architecture</em></a>.</p>\n\n<hr>\n\n<p>If a complex S3 configuration is undesirable, the complexity can instead be shifted into the Terraform workflow by using <a href=\"https://www.terraform.io/docs/backends/config.html#partial-configuration\" rel=\"nofollow noreferrer\">partial configuration</a>. In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running <code>terraform init</code>.</p>\n\n<p>This allows options to vary between runs, but since it requires extra arguments to be provided most organizations adopting this approach will use a <em>wrapper script</em> to configure Terraform appropriately based on local conventions. This can be just a simple shell script that runs <code>terraform init</code> with suitable arguments.</p>\n\n<p>This then allows to vary, for example, the custom credentials file by providing it on the command line. In this case, <em>state environments</em> are not used, and instead switching between environments requires re-initializing the working directory against a new backend configuration.</p>\n\n<p>The advantage of this solution is that it does not impose any particular restrictions on the use of S3 and DynamoDB, as long as the differences can be represented as CLI options.</p>\n\n<p>The disadvantage is the need for unusual workflow or wrapper scripts to configure Terraform.</p>\n", "OwnerUserId": "281848", "LastEditorUserId": "281848", "LastEditDate": "2018-07-18T21:44:15.480", "LastActivityDate": "2018-07-18T21:44:15.480", "CommentCount": "2", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "89747921", "PostId": "44687215", "Score": "0", "Text": "How does terraform know the difference between what I'm running in my preproduction account and my production account when using option #1? I'm concerned that the state will get corrupted by each others data. Is that all handled by workspaces?", "CreationDate": "2018-07-17T18:25:32.387", "UserId": "264881", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "89794270", "PostId": "44687215", "Score": "1", "Text": "Each workspace has an entirely separate state, so as long as you make sure you have the correct workspace selected when you run `terraform plan` and `terraform apply` it will not have access to the state information for any other workspace. Since I wrote this answer, there is now a full guide to the first option on the Terraform website: https://www.terraform.io/docs/backends/types/s3.html#multi-account-aws-architecture", "CreationDate": "2018-07-18T21:39:48.333", "UserId": "281848", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "149918304", "PostHistoryTypeId": "2", "PostId": "44687215", "RevisionGUID": "1c754ce9-df25-42c8-8445-4b86d95f34d7", "CreationDate": "2017-06-21T22:25:14.840", "UserId": "281848", "Text": "Terraform expects backend configuration to be static, and does not allow it to include interpolated variables as might be true elsewhere in the config due to the need for the backend to be initialized before any other work can be done.\r\n\r\nDue to this, applying the same config multiple times using different AWS accounts can be tricky, but is possible in one of two ways.\r\n\r\n---\r\n\r\nThe lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls.\r\n\r\nOrganizations adopting this strategy will sometimes create the S3 bucket in a separate \"adminstrative\" AWS account, and then grant restrictive access to the individual state objects in the bucket to the specific roles that will run Terraform in each of the other accounts.\r\n\r\nThis solution has the advantage that once it has been set up correctly in S3 Terraform can be used routinely without any unusual workflow: configure the single S3 bucket in the backend, and provide appropriate credentials via environment variables to allow them to vary. Once the backend is initialized, use [state environments](https://www.terraform.io/docs/state/environments.html) (soon to be renamed _workspaces_ in Terraform 0.10) to create a separate state for each of the target environments of a single configuration.\r\n\r\nThe disadvantage is, of course, the need to manage a more-complicated access configuration around S3, rather than simply relying on coarse access control with whole AWS accounts. It is also more challenging with DynamoDB in the mix, since the access controls on DynamoDB are not as flexible.\r\n\r\n---\r\n\r\nIf a complex S3 configuration is undesirable, the complexity can instead be shifted into the Terraform workflow by using [partial configuration](https://www.terraform.io/docs/backends/config.html#partial-configuration). In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running `terraform init`.\r\n\r\nThis allows options to vary between runs, but since it requires extra arguments to be provided most organizations adopting this approach will use a _wrapper script_ to configure Terraform appropriately based on local conventions. This can be just a simple shell script that runs `terraform init` with suitable arguments.\r\n\r\nThis then allows to vary, for example, the custom credentials file by providing it on the command line. In this case, _state environments_ are not used, and instead switching between environments requires re-initializing the working directory against a new backend configuration.\r\n\r\nThe advantage of this solution is that it does not impose any particular restrictions on the use of S3 and DynamoDB, as long as the differences can be represented as CLI options.\r\n\r\nThe disadvantage is the need for unusual workflow or wrapper scripts to configure Terraform.\r\n", "ContentLicense": "CC BY-SA 3.0", "filtered-sentences": [{"source": "Text", "text": "--- The lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls. ", "keywords": ["storage"]}, {"source": "Text", "text": "In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running `terraform init`. ", "keywords": ["billing mode"]}]}, {"Id": "177941278", "PostHistoryTypeId": "5", "PostId": "44687215", "RevisionGUID": "6a642c88-3c4a-4607-a3b6-8d576da969cf", "CreationDate": "2018-07-18T21:44:15.480", "UserId": "281848", "Comment": "Add a link to a longer-form version of my first suggestion from the official website", "Text": "Terraform expects backend configuration to be static, and does not allow it to include interpolated variables as might be true elsewhere in the config due to the need for the backend to be initialized before any other work can be done.\r\n\r\nDue to this, applying the same config multiple times using different AWS accounts can be tricky, but is possible in one of two ways.\r\n\r\n---\r\n\r\nThe lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls.\r\n\r\nOrganizations adopting this strategy will sometimes create the S3 bucket in a separate \"adminstrative\" AWS account, and then grant restrictive access to the individual state objects in the bucket to the specific roles that will run Terraform in each of the other accounts.\r\n\r\nThis solution has the advantage that once it has been set up correctly in S3 Terraform can be used routinely without any unusual workflow: configure the single S3 bucket in the backend, and provide appropriate credentials via environment variables to allow them to vary. Once the backend is initialized, use [workspaces](https://www.terraform.io/docs/state/workspaces.html) (known as \"state environments\" prior to Terraform 0.10) to create a separate state for each of the target environments of a single configuration.\r\n\r\nThe disadvantage is the need to manage a more-complicated access configuration around S3, rather than simply relying on coarse access control with whole AWS accounts. It is also more challenging with DynamoDB in the mix, since the access controls on DynamoDB are not as flexible.\r\n\r\nThere is a more complete description of this option in the Terraform `s3` provider documentation, [_Multi-account AWS Architecture_](https://www.terraform.io/docs/backends/types/s3.html#multi-account-aws-architecture).\r\n\r\n---\r\n\r\nIf a complex S3 configuration is undesirable, the complexity can instead be shifted into the Terraform workflow by using [partial configuration](https://www.terraform.io/docs/backends/config.html#partial-configuration). In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running `terraform init`.\r\n\r\nThis allows options to vary between runs, but since it requires extra arguments to be provided most organizations adopting this approach will use a _wrapper script_ to configure Terraform appropriately based on local conventions. This can be just a simple shell script that runs `terraform init` with suitable arguments.\r\n\r\nThis then allows to vary, for example, the custom credentials file by providing it on the command line. In this case, _state environments_ are not used, and instead switching between environments requires re-initializing the working directory against a new backend configuration.\r\n\r\nThe advantage of this solution is that it does not impose any particular restrictions on the use of S3 and DynamoDB, as long as the differences can be represented as CLI options.\r\n\r\nThe disadvantage is the need for unusual workflow or wrapper scripts to configure Terraform.\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "--- The lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls. ", "keywords": ["storage"]}, {"source": "Text", "text": "There is a more complete description of this option in the Terraform `s3` provider documentation, [_Multi-account AWS Architecture_](https://www.terraform.io/docs/backends/types/s3.html#multi-account-aws-architecture). ", "keywords": ["provider"]}, {"source": "Text", "text": "In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running `terraform init`. ", "keywords": ["billing mode"]}]}], "filtered-sentences": [{"source": "Body", "text": "The lowest-friction way is to create a single S3 bucket and DynamoDB table dedicated to state storage across all environments, and use S3 permissions and/or IAM policies to impose granular access controls. ", "keywords": ["storage"]}, {"source": "Body", "text": "There is a more complete description of this option in the Terraform s3 provider documentation, Multi-account AWS Architecture. ", "keywords": ["provider"]}, {"source": "Body", "text": "In this mode, only a subset of the backend settings are provided in config and additional settings are provided on the command line when running terraform init. ", "keywords": ["billing mode"]}]}], "contains-topic": true, "filtered-sentences": [{"source": "Body", "text": "We have multiple aws accounts, 1 for IAM, 1 for billing, 1 for production, 1 for non-production, 1 for shared services etc... you get where I am going. ", "keywords": ["bill"]}, {"source": "Body", "text": "Is there a way to get the backend configuration setup in such a way that it will use the creds and role configured within the provider? ", "keywords": ["provider"]}, {"source": "Body", "text": "Created a policy to just allow that new user s3:DeleteObject,GetObject,PutObject,ListBucket and dynamodb:* on the specific s3 bucket and dynamodb table. ", "keywords": ["policy"]}]}