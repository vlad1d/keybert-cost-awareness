{"Id": "73465480", "PostTypeId": "1", "AcceptedAnswerId": "73475781", "CreationDate": "2022-08-23T22:04:13.683", "Score": "0", "ViewCount": "309", "Body": "<p>I have a requirement to use terraform to provision identical copies of the same infrastructure in different places for failover purposes. For example, I have 2 Kubernetes clusters A and B; I want to be able to use terraform to provision them both to the identical state. It would be as if there was one terraform plan, and 2 parallel applies to different &quot;destinations&quot; would happen for each apply.</p>\n<p>Using provider aliases comes to mind, but that would require duplicating code for everything. Workspaces aren't a good fit either because each set of infrastructure is a first class citizen that should be in-sync with the other.</p>\n<p>The best I've come up with is to use partial configuration for the backend <a href=\"https://www.terraform.io/language/settings/backends/configuration#partial-configuration\" rel=\"nofollow noreferrer\">https://www.terraform.io/language/settings/backends/configuration#partial-configuration</a> and using variables in the provider block like so:</p>\n<pre><code>provider &quot;kubernetes&quot; {\n  cluster = var.foo\n}\n</code></pre>\n<p>And run terraform using:</p>\n<pre><code>terraform init -backend-config=&quot;baz=bat&quot;\nterraform plan -var &quot;foo=bar&quot;\n</code></pre>\n<p>Using this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables.</p>\n<p>The above would work, but would require a separate init, plan, and apply for each distinct copy of the infrastructure being provisioned. Is that the best that I can hope for, or is there a better approach to combine everything into one workflow?</p>\n<p>EDIT: Adding more context based on a comment below. The scenario is that cluster A is in a less expensive, less reliable datacenter and cluster B is in a more expensive, more reliable datacenter. To save costs, we want to run primarily in the less expensive datacenter, but have fully provisioned infrastructure ready to go if there is an outage in the primary datacenter. We'd keep cluster B artificially too small (to achieve the cost savings) until we lose cluster A, at which point we'd scale out cluster B to manage the full workload.</p>\n", "OwnerUserId": "5859621", "LastEditorUserId": "5859621", "LastEditDate": "2022-08-24T11:54:22.843", "LastActivityDate": "2022-08-24T15:21:03.303", "Title": "Use terraform to create identical parallel infrastructure for failover purposes", "Tags": "<terraform><failover><redundancy>", "AnswerCount": "1", "CommentCount": "2", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "129736743", "PostId": "73465480", "Score": "2", "Text": "What level of failure are you particularly concerned about here? The start of your question talks about failing over between Kubernetes clusters, which made me think you were initially talking about _application-level_ failover, but then later you were talking about backend settings and so it sounds like you're concerned about failing over your _Terraform state storage_, rather than the real infrastructure Terraform is managing. Can you edit your question to say a little more about what sort of process you are trying to create here?", "CreationDate": "2022-08-23T22:26:12.350", "UserId": "281848", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "129748205", "PostId": "73465480", "Score": "0", "Text": "@MartinAtkins edited to add more context.", "CreationDate": "2022-08-24T11:54:40.887", "UserId": "5859621", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "276837984", "PostHistoryTypeId": "2", "PostId": "73465480", "RevisionGUID": "713c1ee3-3d91-4e41-a503-c015cd1f5de9", "CreationDate": "2022-08-23T22:04:13.683", "UserId": "5859621", "Text": "I have a requirement to use terraform to provision identical copies of the same infrastructure in different places for failover purposes. For example, I have 2 Kubernetes clusters A and B; I want to be able to use terraform to provision them both to the identical state. It would be as if there was one terraform plan, and 2 parallel applies to different \"destinations\" would happen for each apply.\r\n\r\nUsing provider aliases comes to mind, but that would require duplicating code for everything. Workspaces aren't a good fit either because each set of infrastructure is a first class citizen that should be in-sync with the other.\r\n\r\nThe best I've come up with is to use partial configuration for the backend https://www.terraform.io/language/settings/backends/configuration#partial-configuration and using variables in the provider block like so:\r\n\r\n    provider \"kubernetes\" {\r\n      cluster = var.foo\r\n    }\r\n\r\nAnd run terraform using:\r\n\r\n    terraform init -backend-config=\"baz=bat\"\r\n    terraform plan -var \"foo=bar\"\r\n\r\nUsing this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables.\r\n\r\nThe above would work, but would require a separate init, plan, and apply for each distinct copy of the infrastructure being provisioned. Is that the best that I can hope for, or is there a better approach to combine everything into one workflow?", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Using provider aliases comes to mind, but that would require duplicating code for everything. ", "keywords": ["provider"]}, {"source": "Text", "text": "The best I've come up with is to use partial configuration for the backend https://www.terraform.io/language/settings/backends/configuration#partial-configuration and using variables in the provider block like so: provider \"kubernetes\" { cluster = var.foo } ", "keywords": ["provider", "cluster"]}, {"source": "Text", "text": "Using this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables. ", "keywords": ["provider"]}]}, {"Id": "276837986", "PostHistoryTypeId": "1", "PostId": "73465480", "RevisionGUID": "713c1ee3-3d91-4e41-a503-c015cd1f5de9", "CreationDate": "2022-08-23T22:04:13.683", "UserId": "5859621", "Text": "Use terraform to create identical parallel infrastructure for failover purposes", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "276837987", "PostHistoryTypeId": "3", "PostId": "73465480", "RevisionGUID": "713c1ee3-3d91-4e41-a503-c015cd1f5de9", "CreationDate": "2022-08-23T22:04:13.683", "UserId": "5859621", "Text": "<terraform><failover><redundancy>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "276875854", "PostHistoryTypeId": "5", "PostId": "73465480", "RevisionGUID": "8d603159-b51c-4743-8710-29d4aac537af", "CreationDate": "2022-08-24T11:54:22.843", "UserId": "5859621", "Comment": "added 543 characters in body", "Text": "I have a requirement to use terraform to provision identical copies of the same infrastructure in different places for failover purposes. For example, I have 2 Kubernetes clusters A and B; I want to be able to use terraform to provision them both to the identical state. It would be as if there was one terraform plan, and 2 parallel applies to different \"destinations\" would happen for each apply.\r\n\r\nUsing provider aliases comes to mind, but that would require duplicating code for everything. Workspaces aren't a good fit either because each set of infrastructure is a first class citizen that should be in-sync with the other.\r\n\r\nThe best I've come up with is to use partial configuration for the backend https://www.terraform.io/language/settings/backends/configuration#partial-configuration and using variables in the provider block like so:\r\n\r\n    provider \"kubernetes\" {\r\n      cluster = var.foo\r\n    }\r\n\r\nAnd run terraform using:\r\n\r\n    terraform init -backend-config=\"baz=bat\"\r\n    terraform plan -var \"foo=bar\"\r\n\r\nUsing this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables.\r\n\r\nThe above would work, but would require a separate init, plan, and apply for each distinct copy of the infrastructure being provisioned. Is that the best that I can hope for, or is there a better approach to combine everything into one workflow?\r\n\r\nEDIT: Adding more context based on a comment below. The scenario is that cluster A is in a less expensive, less reliable datacenter and cluster B is in a more expensive, more reliable datacenter. To save costs, we want to run primarily in the less expensive datacenter, but have fully provisioned infrastructure ready to go if there is an outage in the primary datacenter. We'd keep cluster B artificially too small (to achieve the cost savings) until we lose cluster A, at which point we'd scale out cluster B to manage the full workload.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Using provider aliases comes to mind, but that would require duplicating code for everything. ", "keywords": ["provider"]}, {"source": "Text", "text": "The best I've come up with is to use partial configuration for the backend https://www.terraform.io/language/settings/backends/configuration#partial-configuration and using variables in the provider block like so: provider \"kubernetes\" { cluster = var.foo } ", "keywords": ["provider", "cluster"]}, {"source": "Text", "text": "Using this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables. ", "keywords": ["provider"]}, {"source": "Text", "text": "The scenario is that cluster A is in a less expensive, less reliable datacenter and cluster B is in a more expensive, more reliable datacenter. ", "keywords": ["expense", "cluster"]}, {"source": "Text", "text": "To save costs, we want to run primarily in the less expensive datacenter, but have fully provisioned infrastructure ready to go if there is an outage in the primary datacenter. ", "keywords": ["expense"]}, {"source": "Text", "text": "We'd keep cluster B artificially too small (to achieve the cost savings) until we lose cluster A, at which point we'd scale out cluster B to manage the full workload.", "keywords": ["cost", "cluster"]}]}], "answers": [{"Id": "73475781", "PostTypeId": "2", "ParentId": "73465480", "CreationDate": "2022-08-24T15:21:03.303", "Score": "1", "Body": "<p>The situation you are describing sounds like a variation on the typical idea of &quot;environments&quot; where you have two independent <em>production</em> environments, rather than e.g. separate stating and production stages.</p>\n<p>The good news is that you can therefore employ mostly the same strategy that's typical for multiple deployment stages: factor out your common infrastructure into a shared module and write two different configurations that refer to it with some different settings.</p>\n<p>Each of your configurations will presumably consist just of a backend configuration, a provider configuration, and a call to the shared module, like this:</p>\n<pre><code>terraform {\n  backend &quot;example&quot; {\n    # ...\n  }\n\n  required_providers {\n    kubernetes = {\n      source = &quot;hashicorp/kubernetes&quot;\n    }\n  }\n}\n\nprovider &quot;kubernetes&quot; {\n  cluster = &quot;whichever-cluster-is-appropriate-here&quot;\n}\n\nmodule &quot;main&quot; {\n  source = &quot;../modules/main&quot;\n\n  # (whatever settings make sense for this environment)\n}\n</code></pre>\n<p>This structure keeps all of the per-environment settings together in a single configuration, so you can just switch into this directory and run the normal Terraform commands (with no unusual extra options) to update that particular environment.</p>\n<hr />\n<p>From your description it seems like a key requirement here is that each of your environments is a separate failure domain and that's one of the typical reasons to split infrastructure into two separate configurations. Doing so will help ensure that an outage of the underlying platform infrastructure in one environment cannot prevent you from using Terraform to manage the other environment.</p>\n<p>If you intend to build automation around your Terraform runs (which I'd recommend) I'd suggest configuring your automation so that any change to the shared module will automatically trigger a run for both of your environments, just so you can make sure they're always routinely getting updated and thus you won't end up in an awkward situation where you try to fail over and find that the backup environment is &quot;stale&quot; and needs significant updates before you could fail over into it.</p>\n<p>Of course, you'd need to make sure that a failure of one of those runs cannot block applying the other one, because otherwise you will have combined the failure domains together and could prevent yourself from failing over in the event of an outage. The way I would imagine it working (in principle) is that, if there is an outage:</p>\n<ol>\n<li>You change the configuration of the backup environment to increase its scale.</li>\n<li>That triggers a run <em>only for the backup environment</em>, because the shared module hasn't changed. You can apply that to scale up the backup environment.</li>\n<li>You change some setting outside of the scope of both of these environments to redirect incoming requests into the backup environment until the outage is over.</li>\n</ol>\n<p>In the event that you <em>do</em> end up needing to change the shared module during an outage, the flow is similar except that step 2 would trigger a run for each of the environments and the primary environment's run would fail, but you can ignore that for now and just apply the backup environment changes. Once the outage is over, you can re-run the primary environment's run to &quot;catch up&quot; with the changes made in the backup environment before you flip back to the primary environment again, and then scale the backup environment back down.</p>\n<p>The key theme here is that Terraform is a building block of a solution here but is not the entire solution itself: Terraform can help you make the changes you need to make, but you will need to build your own workflow (automated or not) around Terraform to make sure that Terraform is running in the appropriate context at the appropriate time to respond to an outage.</p>\n", "OwnerUserId": "281848", "LastActivityDate": "2022-08-24T15:21:03.303", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "276892313", "PostHistoryTypeId": "2", "PostId": "73475781", "RevisionGUID": "d38410b9-4bc8-4b08-ad0b-3686e4260ff9", "CreationDate": "2022-08-24T15:21:03.303", "UserId": "281848", "Text": "The situation you are describing sounds like a variation on the typical idea of \"environments\" where you have two independent _production_ environments, rather than e.g. separate stating and production stages.\r\n\r\nThe good news is that you can therefore employ mostly the same strategy that's typical for multiple deployment stages: factor out your common infrastructure into a shared module and write two different configurations that refer to it with some different settings.\r\n\r\nEach of your configurations will presumably consist just of a backend configuration, a provider configuration, and a call to the shared module, like this:\r\n\r\n```\r\nterraform {\r\n  backend \"example\" {\r\n    # ...\r\n  }\r\n\r\n  required_providers {\r\n    kubernetes = {\r\n      source = \"hashicorp/kubernetes\"\r\n    }\r\n  }\r\n}\r\n\r\nprovider \"kubernetes\" {\r\n  cluster = \"whichever-cluster-is-appropriate-here\"\r\n}\r\n\r\nmodule \"main\" {\r\n  source = \"../modules/main\"\r\n\r\n  # (whatever settings make sense for this environment)\r\n}\r\n```\r\n\r\nThis structure keeps all of the per-environment settings together in a single configuration, so you can just switch into this directory and run the normal Terraform commands (with no unusual extra options) to update that particular environment.\r\n\r\n---\r\n\r\nFrom your description it seems like a key requirement here is that each of your environments is a separate failure domain and that's one of the typical reasons to split infrastructure into two separate configurations. Doing so will help ensure that an outage of the underlying platform infrastructure in one environment cannot prevent you from using Terraform to manage the other environment.\r\n\r\nIf you intend to build automation around your Terraform runs (which I'd recommend) I'd suggest configuring your automation so that any change to the shared module will automatically trigger a run for both of your environments, just so you can make sure they're always routinely getting updated and thus you won't end up in an awkward situation where you try to fail over and find that the backup environment is \"stale\" and needs significant updates before you could fail over into it.\r\n\r\nOf course, you'd need to make sure that a failure of one of those runs cannot block applying the other one, because otherwise you will have combined the failure domains together and could prevent yourself from failing over in the event of an outage. The way I would imagine it working (in principle) is that, if there is an outage:\r\n\r\n1. You change the configuration of the backup environment to increase its scale.\r\n2. That triggers a run _only for the backup environment_, because the shared module hasn't changed. You can apply that to scale up the backup environment.\r\n3. You change some setting outside of the scope of both of these environments to redirect incoming requests into the backup environment until the outage is over.\r\n\r\nIn the event that you _do_ end up needing to change the shared module during an outage, the flow is similar except that step 2 would trigger a run for each of the environments and the primary environment's run would fail, but you can ignore that for now and just apply the backup environment changes. Once the outage is over, you can re-run the primary environment's run to \"catch up\" with the changes made in the backup environment before you flip back to the primary environment again, and then scale the backup environment back down.\r\n\r\nThe key theme here is that Terraform is a building block of a solution here but is not the entire solution itself: Terraform can help you make the changes you need to make, but you will need to build your own workflow (automated or not) around Terraform to make sure that Terraform is running in the appropriate context at the appropriate time to respond to an outage.\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Each of your configurations will presumably consist just of a backend configuration, a provider configuration, and a call to the shared module, like this: ``` terraform { backend \"example\" { # ... } required_providers { kubernetes = { source = \"hashicorp/kubernetes\" } } } provider \"kubernetes\" { cluster = \"whichever-cluster-is-appropriate-here\" } module \"main\" { source = \"../modules/main\" # (whatever settings make sense for this environment) } ``` ", "keywords": ["provider", "cluster"]}, {"source": "Text", "text": "--- From your description it seems like a key requirement here is that each of your environments is a separate failure domain and that's one of the typical reasons to split infrastructure into two separate configurations. ", "keywords": ["domain"]}, {"source": "Text", "text": "If you intend to build automation around your Terraform runs (which I'd recommend) I'd suggest configuring your automation so that any change to the shared module will automatically trigger a run for both of your environments, just so you can make sure they're always routinely getting updated and thus you won't end up in an awkward situation where you try to fail over and find that the backup environment is \"stale\" and needs significant updates before you could fail over into it. ", "keywords": ["change"]}, {"source": "Text", "text": "The way I would imagine it working (in principle) is that, if there is an outage: 1. You change the configuration of the backup environment to increase its scale. ", "keywords": ["change"]}, {"source": "Text", "text": "2. That triggers a run _only for the backup environment_, because the shared module hasn't changed. ", "keywords": ["change"]}, {"source": "Text", "text": "3. You change some setting outside of the scope of both of these environments to redirect incoming requests into the backup environment until the outage is over. ", "keywords": ["change"]}, {"source": "Text", "text": "In the event that you _do_ end up needing to change the shared module during an outage, the flow is similar except that step 2 would trigger a run for each of the environments and the primary environment's run would fail, but you can ignore that for now and just apply the backup environment changes. ", "keywords": ["change"]}, {"source": "Text", "text": "Once the outage is over, you can re-run the primary environment's run to \"catch up\" with the changes made in the backup environment before you flip back to the primary environment again, and then scale the backup environment back down. ", "keywords": ["change"]}, {"source": "Text", "text": "The key theme here is that Terraform is a building block of a solution here but is not the entire solution itself: Terraform can help you make the changes you need to make, but you will need to build your own workflow (automated or not) around Terraform to make sure that Terraform is running in the appropriate context at the appropriate time to respond to an outage.", "keywords": ["change"]}]}], "filtered-sentences": [{"source": "Body", "text": "Each of your configurations will presumably consist just of a backend configuration, a provider configuration, and a call to the shared module, like this: ", "keywords": ["provider"]}, {"source": "Body", "text": "From your description it seems like a key requirement here is that each of your environments is a separate failure domain and that's one of the typical reasons to split infrastructure into two separate configurations. ", "keywords": ["domain"]}, {"source": "Body", "text": "I'd suggest configuring your automation so that any change to the shared module will automatically trigger a run for both of your environments, just so you can make sure they're always routinely getting updated and thus you won't end up in an awkward situation where you try to fail over and find that the backup environment is \"stale\" and needs significant updates before you could fail over into it. ", "keywords": ["change"]}, {"source": "Body", "text": "The way I would imagine it working (in principle) is that, if there is an outage: You change the configuration of the backup environment to increase its scale. ", "keywords": ["change"]}, {"source": "Body", "text": "That triggers a run only for the backup environment, because the shared module hasn't changed. ", "keywords": ["change"]}, {"source": "Body", "text": "You change some setting outside of the scope of both of these environments to redirect incoming requests into the backup environment until the outage is over. ", "keywords": ["change"]}, {"source": "Body", "text": "In the event that you do end up needing to change the shared module during an outage, the flow is similar except that step 2 would trigger a run for each of the environments and the primary environment's run would fail, but you can ignore that for now and just apply the backup environment changes. ", "keywords": ["change"]}, {"source": "Body", "text": "Once the outage is over, you can re-run the primary environment's run to \"catch up\" with the changes made in the backup environment before you flip back to the primary environment again, and then scale the backup environment back down. ", "keywords": ["change"]}, {"source": "Body", "text": "The key theme here is that Terraform is a building block of a solution here but is not the entire solution itself: Terraform can help you make the changes you need to make, but you will need to build your own workflow (automated or not) around Terraform to make sure that Terraform is running in the appropriate context at the appropriate time to respond to an outage.", "keywords": ["change"]}]}], "contains-topic": true, "filtered-sentences": [{"source": "Body", "text": "Using provider aliases comes to mind, but that would require duplicating code for everything. ", "keywords": ["provider"]}, {"source": "Body", "text": "The best I've come up with is to use partial configuration for the backend https://www.terraform.io/language/settings/backends/configuration#partial-configuration and using variables in the provider block like so: And run terraform using: Using this approach, there's a separate backend state for each copy of the infrastructure, and the provider will be pointed to the right destination via command line variables. ", "keywords": ["provider"]}, {"source": "Body", "text": "The scenario is that cluster A is in a less expensive, less reliable datacenter and cluster B is in a more expensive, more reliable datacenter. ", "keywords": ["expense", "cluster"]}, {"source": "Body", "text": "To save costs, we want to run primarily in the less expensive datacenter, but have fully provisioned infrastructure ready to go if there is an outage in the primary datacenter. ", "keywords": ["expense"]}, {"source": "Body", "text": "We'd keep cluster B artificially too small (to achieve the cost savings) until we lose cluster A, at which point we'd scale out cluster B to manage the full workload.", "keywords": ["cost", "cluster"]}]}