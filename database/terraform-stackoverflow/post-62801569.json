{"Id": "62801569", "PostTypeId": "1", "AcceptedAnswerId": "62801796", "CreationDate": "2020-07-08T18:42:10.707", "Score": "1", "ViewCount": "3341", "Body": "<p>I am using terraform to manage aws environments for our application.  The environments have s3 buckets for various things.  And when setting up a new environment I just want to copy the buckets from a base source bucket, or from an existing environment.</p>\n<p>But I can't find anything that will provision a copy.  The AWS interface lets you duplicate the setting when creating (which I don't need), but not the objects, so it may not be something terraform can do directly.</p>\n<p>If so, how about indirectly?</p>\n", "OwnerUserId": "5543950", "LastEditorUserId": "13460933", "LastEditDate": "2020-07-08T18:58:25.527", "LastActivityDate": "2020-07-09T00:10:21.700", "Title": "Can terraform duplicate the content of an s3 bucket?", "Tags": "<amazon-web-services><amazon-s3><terraform>", "AnswerCount": "2", "CommentCount": "2", "FavoriteCount": "0", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "111058288", "PostId": "62801569", "Score": "1", "Text": "Just to confirm is the request to copy the contents of an S3 bucket to a new S3 bucket via Terraform?", "CreationDate": "2020-07-08T18:47:05.957", "UserId": "13460933", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "111058487", "PostId": "62801569", "Score": "0", "Text": "Yes, overall to create a new bucket and have the same contents as an existing bucket.  If that means creating the new bucket first and then somehow copying the contents of a source bucket into the new bucket, then so be it.", "CreationDate": "2020-07-08T18:53:19.640", "UserId": "5543950", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "226176786", "PostHistoryTypeId": "2", "PostId": "62801569", "RevisionGUID": "f7e0374f-4a0e-411b-8db4-ffd097766b8b", "CreationDate": "2020-07-08T18:42:10.707", "UserId": "5543950", "Text": "I am using terraform to manage aws environments for our application.  The environments have s3 buckets for various things.  And when setting up a new environment I just want to copy the buckets from a base source bucket, or from an existing environment.  But I can't find anything that will provision a copy.  The AWS interface lets you duplicate the setting when creating (which I don't need), but not the objects, so it may not be something terraform can do directly.  If so, how about indirectly?    ", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "226176787", "PostHistoryTypeId": "1", "PostId": "62801569", "RevisionGUID": "f7e0374f-4a0e-411b-8db4-ffd097766b8b", "CreationDate": "2020-07-08T18:42:10.707", "UserId": "5543950", "Text": "Can terraform duplicate an s3 bucket?", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "226176788", "PostHistoryTypeId": "3", "PostId": "62801569", "RevisionGUID": "f7e0374f-4a0e-411b-8db4-ffd097766b8b", "CreationDate": "2020-07-08T18:42:10.707", "UserId": "5543950", "Text": "<amazon-s3><terraform>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "226176902", "PostHistoryTypeId": "5", "PostId": "62801569", "RevisionGUID": "474bec93-5fa3-4d48-bd55-7349a475d306", "CreationDate": "2020-07-08T18:43:55.153", "UserId": "13460933", "Comment": "added 4 characters in body", "Text": "I am using terraform to manage aws environments for our application.  The environments have s3 buckets for various things.  And when setting up a new environment I just want to copy the buckets from a base source bucket, or from an existing environment.\r\n\r\nBut I can't find anything that will provision a copy.  The AWS interface lets you duplicate the setting when creating (which I don't need), but not the objects, so it may not be something terraform can do directly.\r\n\r\nIf so, how about indirectly?    ", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "226177965", "PostHistoryTypeId": "6", "PostId": "62801569", "RevisionGUID": "1198cf1d-29a3-4345-9aba-a65a38560139", "CreationDate": "2020-07-08T18:58:25.527", "UserId": "13460933", "Comment": "edited tags", "Text": "<amazon-web-services><amazon-s3><terraform>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "226177980", "PostHistoryTypeId": "4", "PostId": "62801569", "RevisionGUID": "1198cf1d-29a3-4345-9aba-a65a38560139", "CreationDate": "2020-07-08T18:58:25.527", "UserId": "13460933", "Comment": "edited tags", "Text": "Can terraform duplicate the content of an s3 bucket?", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "answers": [{"Id": "62801796", "PostTypeId": "2", "ParentId": "62801569", "CreationDate": "2020-07-08T18:56:32.453", "Score": "6", "Body": "<p>There is no resource that enables the copying of objects from one S3 bucket to another. If you want to include this in your Terraform setup then you would need to use a <a href=\"https://www.terraform.io/docs/provisioners/local-exec.html\" rel=\"noreferrer\">local-exec</a> provisioner.</p>\n<p>It would need to execute the command below, with the support the AWS CLI to run <code>aws s3 cp</code>.</p>\n<pre><code>resource &quot;null_resource&quot; &quot;s3_objects&quot; {\n  provisioner &quot;local-exec&quot; {\n    command = &quot;aws s3 cp s3://bucket1 s3://bucket2 --recursive&quot;\n  }\n}\n</code></pre>\n<p>For this to run the local server would need to have the <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\" rel=\"noreferrer\">AWS CLI installed</a> with a role (or valid credentials) to enable the copy.</p>\n", "OwnerUserId": "13460933", "LastActivityDate": "2020-07-08T18:56:32.453", "CommentCount": "4", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "111058702", "PostId": "62801796", "Score": "0", "Text": "interesting.  I think I can make that work.  Locally it will be easy... the hard part will be getting it to work inside teamcity... :)  But that is entirely an internal issue.  Thanks.", "CreationDate": "2020-07-08T19:01:15.933", "UserId": "5543950", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "111058727", "PostId": "62801796", "Score": "0", "Text": "No problem, glad I could help :)", "CreationDate": "2020-07-08T19:01:44.493", "UserId": "13460933", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "120886379", "PostId": "62801796", "Score": "0", "Text": "This may no longer be accurate. I noticed this today: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object_copy - it seems to do the object copying via TF, but follows along with the TF 'way of doing things' in that it only copies if there's some sort of need for that copy to happen.", "CreationDate": "2021-07-15T21:55:45.430", "UserId": "909135", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "120891662", "PostId": "62801796", "Score": "0", "Text": "Perhaps although bare in mind this is an object by object copy as opposed to the entire contents of the bucket", "CreationDate": "2021-07-16T06:27:26.300", "UserId": "13460933", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "226177812", "PostHistoryTypeId": "2", "PostId": "62801796", "RevisionGUID": "0b26f03e-03fc-4c8d-b08a-9d6fa0e1281a", "CreationDate": "2020-07-08T18:56:32.453", "UserId": "13460933", "Text": "There is no resource that enables the copying of objects from one S3 bucket to another. If you want to include this in your Terraform setup then you would need to use a [local-exec](https://www.terraform.io/docs/provisioners/local-exec.html) provisioner.\r\n\r\nIt would need to execute the command below, with the support the AWS CLI to run `aws s3 cp`.\r\n\r\n```\r\nresource \"null_resource\" \"s3_objects\" {\r\n  provisioner \"local-exec\" {\r\n    command = \"aws s3 cp s3://bucket1 s3://bucket2 --recursive\"\r\n  }\r\n}\r\n```\r\n\r\nFor this to run the local server would need to have the [AWS CLI installed](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) with a role (or valid credentials) to enable the copy.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "filtered-sentences": []}, {"Id": "62805462", "PostTypeId": "2", "ParentId": "62801569", "CreationDate": "2020-07-09T00:10:21.700", "Score": "3", "Body": "<p>Generally-speaking, Terraform providers reflect operations that are natively supported by the underlying APIs, but in some cases we can use various Terraform resource types together to achieve functionality that the underlying provider lacks.</p>\n<p>I believe there's no native S3 operation for bulk-copying objects from one bucket to another, so to solve this with Terraform requires decomposing the problem into smaller steps, which I think in this case would be:</p>\n<ul>\n<li>Declare a new bucket, the target</li>\n<li>List all of the objects in the source bucket</li>\n<li>Declare one object in the new bucket per object in the source bucket.</li>\n</ul>\n<p>The AWS provider can in principle do all three of these operations: it has managed resource types for both buckets and bucket objects, and it has a data source <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/s3_bucket_objects\" rel=\"nofollow noreferrer\"><code>aws_s3_bucket_objects</code></a> which can enumerate some or all of the objects in a bucket.</p>\n<p>We can combine those pieces together in a Terraform configuration like this:</p>\n<pre><code>resource &quot;aws_s3_bucket&quot; &quot;target&quot; {\n  bucket = &quot;copy-example-target&quot;\n}\n\ndata &quot;aws_s3_bucket_objects&quot; &quot;source&quot; {\n  bucket = &quot;copy-example-source&quot;\n}\n\ndata &quot;aws_s3_bucket_object&quot; &quot;source&quot; {\n  for_each = toset(data.aws_s3_bucket_objects.source.keys)\n\n  bucket = data.aws_s3_bucket_objects.source.bucket\n  key    = each.key\n}\n\nresource &quot;aws_s3_bucket_object&quot; &quot;target&quot; {\n  for_each = aws_s3_bucket_object.source\n\n  bucket  = aws_s3_bucket.target.bucket\n  key     = each.key\n  content = each.value.body\n}\n</code></pre>\n<p>With that said, Terraform is likely not the best tool to for this situation for the following reasons:</p>\n<ul>\n<li>The above configuration will cause Terraform to read all of the objects in the bucket into memory, which would be time consuming and use lots of RAM for larger buckets, and then ultimately store all of them in the Terraform state, which would make the state itself very large.</li>\n<li>Because the <code>aws_s3_bucket_object</code> data source is intended mainly for retrieving small text-based objects, the above will work only if everything in the bucket meets the limitations described in <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/s3_bucket_object\" rel=\"nofollow noreferrer\">the <code>aws_s3_bucket_object</code> documentation</a>: the objects must all have text-indicating MIME types and they must all contain UTF-8 encoded text.</li>\n</ul>\n<p>In this case then, I would prefer to use a specialized tool for the job which is designed to exploit all of the features of the S3 API to make the copy as efficient as possible, such as streaming the list of objects and streaming the contents of each object in chunks to avoid the need to have all of the data in memory at once. One such tool is in the AWS CLI itself, in the form of the <code>aws s3 cp</code> command with the <code>--recursive</code> option.</p>\n", "OwnerUserId": "281848", "LastActivityDate": "2020-07-09T00:10:21.700", "CommentCount": "1", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "111092563", "PostId": "62805462", "Score": "0", "Text": "Thanks for the info... Yeah, I had thought something like this might be possible. But worried that it would also cost a lot since the files would have to actually move to the running machine.  My hope is that the AWS cli command to cp doesn't actually bring the files to the running host.  But I wasn't able to find any 100% proof of that while googling.  That said, I hadn't even considered that the file contents would end up in the statefile.  One of the buckets is massive... took hours for the cli to copy.", "CreationDate": "2020-07-09T18:32:57.750", "UserId": "5543950", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "But worried that it would also cost a lot since the files would have to actually move to the running machine. ", "keywords": ["cost"]}]}], "history": [{"Id": "226192651", "PostHistoryTypeId": "2", "PostId": "62805462", "RevisionGUID": "685b6d71-f443-4193-b89a-87479be9857b", "CreationDate": "2020-07-09T00:10:21.700", "UserId": "281848", "Text": "Generally-speaking, Terraform providers reflect operations that are natively supported by the underlying APIs, but in some cases we can use various Terraform resource types together to achieve functionality that the underlying provider lacks.\r\n\r\nI believe there's no native S3 operation for bulk-copying objects from one bucket to another, so to solve this with Terraform requires decomposing the problem into smaller steps, which I think in this case would be:\r\n\r\n* Declare a new bucket, the target\r\n* List all of the objects in the source bucket\r\n* Declare one object in the new bucket per object in the source bucket.\r\n\r\nThe AWS provider can in principle do all three of these operations: it has managed resource types for both buckets and bucket objects, and it has a data source [`aws_s3_bucket_objects`](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/s3_bucket_objects) which can enumerate some or all of the objects in a bucket.\r\n\r\nWe can combine those pieces together in a Terraform configuration like this:\r\n\r\n```\r\nresource \"aws_s3_bucket\" \"target\" {\r\n  bucket = \"copy-example-target\"\r\n}\r\n\r\ndata \"aws_s3_bucket_objects\" \"source\" {\r\n  bucket = \"copy-example-source\"\r\n}\r\n\r\ndata \"aws_s3_bucket_object\" \"source\" {\r\n  for_each = toset(data.aws_s3_bucket_objects.source.keys)\r\n\r\n  bucket = data.aws_s3_bucket_objects.source.bucket\r\n  key    = each.key\r\n}\r\n\r\nresource \"aws_s3_bucket_object\" \"target\" {\r\n  for_each = aws_s3_bucket_object.source\r\n\r\n  bucket  = aws_s3_bucket.target.bucket\r\n  key     = each.key\r\n  content = each.value.body\r\n}\r\n```\r\n\r\nWith that said, Terraform is likely not the best tool to for this situation for the following reasons:\r\n\r\n* The above configuration will cause Terraform to read all of the objects in the bucket into memory, which would be time consuming and use lots of RAM for larger buckets, and then ultimately store all of them in the Terraform state, which would make the state itself very large.\r\n* Because the `aws_s3_bucket_object` data source is intended mainly for retrieving small text-based objects, the above will work only if everything in the bucket meets the limitations described in [the `aws_s3_bucket_object` documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/s3_bucket_object): the objects must all have text-indicating MIME types and they must all contain UTF-8 encoded text.\r\n\r\nIn this case then, I would prefer to use a specialized tool for the job which is designed to exploit all of the features of the S3 API to make the copy as efficient as possible, such as streaming the list of objects and streaming the contents of each object in chunks to avoid the need to have all of the data in memory at once. One such tool is in the AWS CLI itself, in the form of the `aws s3 cp` command with the `--recursive` option.\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Generally-speaking, Terraform providers reflect operations that are natively supported by the underlying APIs, but in some cases we can use various Terraform resource types together to achieve functionality that the underlying provider lacks. ", "keywords": ["provider"]}, {"source": "Text", "text": "The AWS provider can in principle do all three of these operations: it has managed resource types for both buckets and bucket objects, and it has a data source [`aws_s3_bucket_objects`](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/s3_bucket_objects) which can enumerate some or all of the objects in a bucket. ", "keywords": ["provider"]}, {"source": "Text", "text": "The above configuration will cause Terraform to read all of the objects in the bucket into memory, which would be time consuming and use lots of RAM for larger buckets, and then ultimately store all of them in the Terraform state, which would make the state itself very large. ", "keywords": ["ram"]}, {"source": "Text", "text": "In this case then, I would prefer to use a specialized tool for the job which is designed to exploit all of the features of the S3 API to make the copy as efficient as possible, such as streaming the list of objects and streaming the contents of each object in chunks to avoid the need to have all of the data in memory at once. ", "keywords": ["efficient"]}]}], "filtered-sentences": [{"source": "Body", "text": "Generally-speaking, Terraform providers reflect operations that are natively supported by the underlying APIs, but in some cases we can use various Terraform resource types together to achieve functionality that the underlying provider lacks. ", "keywords": ["provider"]}, {"source": "Body", "text": "The AWS provider can in principle do all three of these operations: it has managed resource types for both buckets and bucket objects, and it has a data source aws_s3_bucket_objects which can enumerate some or all of the objects in a bucket. ", "keywords": ["provider"]}, {"source": "Body", "text": "The above configuration will cause Terraform to read all of the objects in the bucket into memory, which would be time consuming and use lots of RAM for larger buckets, and then ultimately store all of them in the Terraform state, which would make the state itself very large. ", "keywords": ["ram"]}, {"source": "Body", "text": "In this case then, I would prefer to use a specialized tool for the job which is designed to exploit all of the features of the S3 API to make the copy as efficient as possible, such as streaming the list of objects and streaming the contents of each object in chunks to avoid the need to have all of the data in memory at once. ", "keywords": ["efficient"]}]}], "contains-topic": true, "filtered-sentences": []}