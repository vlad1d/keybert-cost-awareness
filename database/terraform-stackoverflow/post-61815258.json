{"Id": "61815258", "PostTypeId": "1", "AcceptedAnswerId": "61815726", "CreationDate": "2020-05-15T08:46:05.690", "Score": "3", "ViewCount": "2173", "Body": "<p>I am deploying AWS Elastic Kubernetes Cluster on AWS Cloud. While deploying the cluster from my local machine I am facing a small error, even we can't say exactly it is an error. </p>\n\n<p>So when I am deploying eks cluster using terraform charts from my local machine, it's deploying all the infra requirement on AWS, but when it has to deploy the cluster it is tying to deploy through <strong>kubectl</strong>, but kubectl is not configured with the newly created cluster, then the terraform throwing an error.</p>\n\n<p>I easily solve this error by binding kubectl with newly created cluster with the below command, but I don't want to do it manually, is there any way in then that I can configure kubectl with the same.</p>\n\n<p>Command - <strong>aws eks --region us-west-2 update-kubeconfig --name clustername</strong></p>\n\n<p>FYI - I am using AWS CLI.</p>\n", "OwnerUserId": "13385253", "LastActivityDate": "2021-02-11T16:35:22.953", "Title": "Terraform AWS EKS kubectl configuration", "Tags": "<amazon-web-services><kubernetes-helm><terraform-provider-aws><amazon-eks>", "AnswerCount": "2", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "221705157", "PostHistoryTypeId": "2", "PostId": "61815258", "RevisionGUID": "b43931d7-1ffe-4afd-8e5b-232728a342c0", "CreationDate": "2020-05-15T08:46:05.690", "UserId": "13385253", "Text": "I am deploying AWS Elastic Kubernetes Cluster on AWS Cloud. While deploying the cluster from my local machine I am facing a small error, even we can't say exactly it is an error. \r\n\r\nSo when I am deploying eks cluster using terraform charts from my local machine, it's deploying all the infra requirement on AWS, but when it has to deploy the cluster it is tying to deploy through **kubectl**, but kubectl is not configured with the newly created cluster, then the terraform throwing an error.\r\n\r\nI easily solve this error by binding kubectl with newly created cluster with the below command, but I don't want to do it manually, is there any way in then that I can configure kubectl with the same.\r\n\r\nCommand - **aws eks --region us-west-2 update-kubeconfig --name clustername**\r\n\r\nFYI - I am using AWS CLI.\r\n\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "I am deploying AWS Elastic Kubernetes Cluster on AWS Cloud. ", "keywords": ["cluster"]}, {"source": "Text", "text": "While deploying the cluster from my local machine I am facing a small error, even we can't say exactly it is an error. ", "keywords": ["cluster"]}, {"source": "Text", "text": "So when I am deploying eks cluster using terraform charts from my local machine, it's deploying all the infra requirement on AWS, but when it has to deploy the cluster it is tying to deploy through **kubectl**, but kubectl is not configured with the newly created cluster, then the terraform throwing an error. ", "keywords": ["cluster"]}, {"source": "Text", "text": "I easily solve this error by binding kubectl with newly created cluster with the below command, but I don't want to do it manually, is there any way in then that I can configure kubectl with the same. ", "keywords": ["cluster"]}]}, {"Id": "221705158", "PostHistoryTypeId": "1", "PostId": "61815258", "RevisionGUID": "b43931d7-1ffe-4afd-8e5b-232728a342c0", "CreationDate": "2020-05-15T08:46:05.690", "UserId": "13385253", "Text": "Terraform AWS EKS kubectl configuration", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "221705159", "PostHistoryTypeId": "3", "PostId": "61815258", "RevisionGUID": "b43931d7-1ffe-4afd-8e5b-232728a342c0", "CreationDate": "2020-05-15T08:46:05.690", "UserId": "13385253", "Text": "<amazon-web-services><kubernetes-helm><terraform-provider-aws><amazon-eks>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "answers": [{"Id": "66158811", "PostTypeId": "2", "ParentId": "61815258", "CreationDate": "2021-02-11T16:35:22.953", "Score": "1", "Body": "<p>You need to have a few things in place:</p>\n<ul>\n<li>dependency on the cluster resource/module: awe_eks_cluster, terraform-aws-modules/eks/aws, etc</li>\n<li>when to regenerate (in specific cases vs always vs never ie generate only once)</li>\n<li>which shell to use (bash as it is most common)</li>\n<li>error exit if fail (set -e)</li>\n<li>wait for cluster to be ready (aws eks wait)</li>\n<li>update kubeconfig</li>\n</ul>\n<p>Eg I use</p>\n<pre><code>resource &quot;null_resource&quot; &quot;merge_kubeconfig&quot; {\n  triggers = {\n    always = timestamp()\n  }\n\n  depends_on = [module.eks_cluster]\n\n  provisioner &quot;local-exec&quot; {\n    interpreter = [&quot;/bin/bash&quot;, &quot;-c&quot;]\n    command = &lt;&lt;EOT\n      set -e\n      echo 'Applying Auth ConfigMap with kubectl...'\n      aws eks wait cluster-active --name '${local.cluster_name}'\n      aws eks update-kubeconfig --name '${local.cluster_name}' --alias '${local.cluster_name}-${var.region}' --region=${var.region}\n    EOT\n  }\n}\n</code></pre>\n<p>Note that there are several things that could cause the kubeconfig to require re-merging such as new cert, new users etc and it can be a bit tricky. The cost is minimal to always re-merge if terraform apply is run, hence trigger on timestamp. Adjust as necessary, eg I have seen this used:</p>\n<pre><code>triggers = {\n    cluster_updated                     = join(&quot;&quot;, aws_eks_cluster.default.*.id)\n    worker_roles_updated                = local.map_worker_roles_yaml\n    additional_roles_updated            = local.map_additional_iam_roles_yaml\n    additional_users_updated            = local.map_additional_iam_users_yaml\n    additional_aws_accounts_updated     = local.map_additional_aws_accounts_yaml\n    configmap_auth_file_content_changed = join(&quot;&quot;, local_file.configmap_auth.*.content)\n    configmap_auth_file_id_changed      = join(&quot;&quot;, local_file.configmap_auth.*.id)\n  }\n</code></pre>\n", "OwnerUserId": "869951", "LastActivityDate": "2021-02-11T16:35:22.953", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "240633310", "PostHistoryTypeId": "2", "PostId": "66158811", "RevisionGUID": "5cc80f54-3b83-499c-a178-7a03549d25d4", "CreationDate": "2021-02-11T16:35:22.953", "UserId": "869951", "Text": "You need to have a few things in place: \r\n- dependency on the cluster resource/module: awe_eks_cluster, terraform-aws-modules/eks/aws, etc\r\n- when to regenerate (in specific cases vs always vs never ie generate only once)\r\n- which shell to use (bash as it is most common)\r\n- error exit if fail (set -e)\r\n- wait for cluster to be ready (aws eks wait)\r\n- update kubeconfig\r\n\r\nEg I use \r\n\r\n    resource \"null_resource\" \"merge_kubeconfig\" {\r\n      triggers = {\r\n        always = timestamp()\r\n      }\r\n    \r\n      depends_on = [module.eks_cluster]\r\n    \r\n      provisioner \"local-exec\" {\r\n        interpreter = [\"/bin/bash\", \"-c\"]\r\n        command = <<EOT\r\n          set -e\r\n          echo 'Applying Auth ConfigMap with kubectl...'\r\n          aws eks wait cluster-active --name '${local.cluster_name}'\r\n          aws eks update-kubeconfig --name '${local.cluster_name}' --alias '${local.cluster_name}-${var.region}' --region=${var.region}\r\n        EOT\r\n      }\r\n    }\r\n\r\nNote that there are several things that could cause the kubeconfig to require re-merging such as new cert, new users etc and it can be a bit tricky. The cost is minimal to always re-merge if terraform apply is run, hence trigger on timestamp. Adjust as necessary, eg I have seen this used: \r\n\r\n    triggers = {\r\n        cluster_updated                     = join(\"\", aws_eks_cluster.default.*.id)\r\n        worker_roles_updated                = local.map_worker_roles_yaml\r\n        additional_roles_updated            = local.map_additional_iam_roles_yaml\r\n        additional_users_updated            = local.map_additional_iam_users_yaml\r\n        additional_aws_accounts_updated     = local.map_additional_aws_accounts_yaml\r\n        configmap_auth_file_content_changed = join(\"\", local_file.configmap_auth.*.content)\r\n        configmap_auth_file_id_changed      = join(\"\", local_file.configmap_auth.*.id)\r\n      }\r\n\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "You need to have a few things in place: - dependency on the cluster resource/module: awe_eks_cluster, terraform-aws-modules/eks/aws, etc - when to regenerate (in specific cases vs always vs never ie generate only once) - which shell to use (bash as it is most common) - error exit if fail (set -e) - wait for cluster to be ready (aws eks wait) - update kubeconfig Eg I use resource \"null_resource\" \"merge_kubeconfig\" { triggers = { always = timestamp() } depends_on = [module.eks_cluster] provisioner \"local-exec\" { interpreter = [\"/bin/bash\", \"-c\"] command = <<EOT set -e echo 'Applying Auth ConfigMap with kubectl...' aws eks wait cluster-active --name '${local.cluster_name}' aws eks update-kubeconfig --name '${local.cluster_name}' --alias '${local.cluster_name}-${var.region}' --region=${var.region} EOT } } Note that there are several things that could cause the kubeconfig to require re-merging such as new cert, new users etc and it can be a bit tricky. ", "keywords": ["cluster"]}, {"source": "Text", "text": "The cost is minimal to always re-merge if terraform apply is run, hence trigger on timestamp. ", "keywords": ["cost"]}]}], "filtered-sentences": [{"source": "Body", "text": "You need to have a few things in place: dependency on the cluster resource/module: awe_eks_cluster, terraform-aws-modules/eks/aws, etc when to regenerate (in specific cases vs always vs never ie generate only once) which shell to use (bash as it is most common) error exit if fail (set -e) wait for cluster to be ready (aws eks wait) update kubeconfig Eg I use Note that there are several things that could cause the kubeconfig to require re-merging such as new cert, new users etc and it can be a bit tricky. ", "keywords": ["cluster"]}, {"source": "Body", "text": "The cost is minimal to always re-merge if terraform apply is run, hence trigger on timestamp. ", "keywords": ["cost"]}]}, {"Id": "61815726", "PostTypeId": "2", "ParentId": "61815258", "CreationDate": "2020-05-15T09:12:17.240", "Score": "3", "Body": "<p>You can use terraform local-exec <a href=\"https://www.terraform.io/docs/provisioners/local-exec.html\" rel=\"nofollow noreferrer\">provisioner</a>. </p>\n\n<pre><code>    resource \"null_resource\" \"kubectl\" {\n       depends_on = &lt;CLUSTER_IS_READY&gt;\n       provisioner \"local-exec\" {\n          command = \"aws eks --region us-west-2 update-kubeconfig --name clustername\"\n          }\n       }\n }\n</code></pre>\n", "OwnerUserId": "3680692", "LastActivityDate": "2020-05-15T09:12:17.240", "CommentCount": "2", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "109372385", "PostId": "61815726", "Score": "0", "Text": "can you please let me know the exact position where I have to put this code", "CreationDate": "2020-05-16T11:58:51.837", "UserId": "13385253", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "109601014", "PostId": "61815726", "Score": "0", "Text": "Thanks @hariK, It's worked. But what happening, the script is still giving error on this stage but now after this configuration it working after just re-run the ***terraform apply*** command.", "CreationDate": "2020-05-23T05:57:09.780", "UserId": "13385253", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "221707121", "PostHistoryTypeId": "2", "PostId": "61815726", "RevisionGUID": "e9f1a30e-d068-4cf0-b45c-bc484c8ccee2", "CreationDate": "2020-05-15T09:12:17.240", "UserId": "3680692", "Text": "You can use terraform local-exec [provisioner][1]. \r\n\r\n\r\n  [1]: https://www.terraform.io/docs/provisioners/local-exec.html\r\n\r\n        resource \"null_resource\" \"kubectl\" {\r\n           depends_on = <CLUSTER_IS_READY>\r\n           provisioner \"local-exec\" {\r\n              command = \"aws eks --region us-west-2 update-kubeconfig --name clustername\"\r\n              }\r\n           }\r\n     }", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "filtered-sentences": []}], "contains-topic": true, "filtered-sentences": [{"source": "Body", "text": "I am deploying AWS Elastic Kubernetes Cluster on AWS Cloud. ", "keywords": ["cluster"]}, {"source": "Body", "text": "While deploying the cluster from my local machine I am facing a small error, even we can't say exactly it is an error. ", "keywords": ["cluster"]}, {"source": "Body", "text": "So when I am deploying eks cluster using terraform charts from my local machine, it's deploying all the infra requirement on AWS, but when it has to deploy the cluster it is tying to deploy through kubectl, but kubectl is not configured with the newly created cluster, then the terraform throwing an error. ", "keywords": ["cluster"]}, {"source": "Body", "text": "I easily solve this error by binding kubectl with newly created cluster with the below command, but I don't want to do it manually, is there any way in then that I can configure kubectl with the same. ", "keywords": ["cluster"]}]}