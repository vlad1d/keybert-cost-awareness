{"Id": "69988728", "PostTypeId": "1", "AcceptedAnswerId": "70096632", "CreationDate": "2021-11-16T11:56:22.937", "Score": "3", "ViewCount": "1906", "Body": "<p>Below is code from my Bigquery terraform main.tf file</p>\n<pre><code>    resource &quot;google_bigquery_data_transfer_config&quot; &quot;config_queries&quot; {\n  for_each = fileset(&quot;${path.module}/scheduled_queries&quot;, &quot;*.sql&quot;)\n  depends_on = [google_bigquery_table.tables]\n  data_source_id = &quot;scheduled_queries&quot;\n  location=var.location\n  destination_dataset_id=google_bigquery_dataset.dataset.dataset_id\n  display_name = &quot;config_queries_${substr(each.value, 0, length(each.value) -4)}&quot;\n  schedule = &quot;every 4 hours &quot;\n params = {\n    destination_table_name_template = substr(each.value,0,length(each.value) -4)\n    write_disposition = &quot;WRITE_APPEND&quot;\n    query = file(&quot;${path.module}/scheduled_queries/${each.value}&quot;)\n  }\n}\n</code></pre>\n<p>I am getting below error:</p>\n<p>\u2502 Error: Error creating Config: googleapi: Error 404: Requested entity was not found.</p>\n<p>The same code is working fine if running sql files individually.\nAny suggestions</p>\n", "OwnerUserId": "7639681", "LastActivityDate": "2023-02-16T22:17:39.400", "Title": "Bigquery Terraform Schedule queries deployment error", "Tags": "<google-bigquery><terraform><terraform-provider-gcp>", "AnswerCount": "2", "CommentCount": "3", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "123756689", "PostId": "69988728", "Score": "1", "Text": "This usually happens when the account you are using wont have access to the resource. Can you confirm if the project exists and have a valid billable account. Also, that's the only error raised? what is the output of \"terraform plan\"?? are you able to generate the execution plan without issues??", "CreationDate": "2021-11-17T17:36:48.570", "UserId": "14551567", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Can you confirm if the project exists and have a valid billable account. ", "keywords": ["bill"]}]}, {"Id": "123780035", "PostId": "69988728", "Score": "0", "Text": "https://github.com/googleapis/python-bigquery-datatransfer/issues/21   ...this link solution worked for me", "CreationDate": "2021-11-18T15:03:30.070", "UserId": "7639681", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "123780143", "PostId": "69988728", "Score": "1", "Text": "I'm glad you were able to solve your issue. To also help future users in the community, the solution was the comment provided by paslandu? Can you detail it? you can respond your own question too and i will thumb up.", "CreationDate": "2021-11-18T15:06:52.650", "UserId": "14551567", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "history": [{"Id": "258091270", "PostHistoryTypeId": "2", "PostId": "69988728", "RevisionGUID": "6a40759c-7454-4f47-a112-bed914d45171", "CreationDate": "2021-11-16T11:56:22.937", "UserId": "7639681", "Text": "Below is code from my Bigquery terraform main.tf file\r\n\r\n        resource \"google_bigquery_data_transfer_config\" \"config_queries\" {\r\n      for_each = fileset(\"${path.module}/scheduled_queries\", \"*.sql\")\r\n      depends_on = [google_bigquery_table.tables]\r\n      data_source_id = \"scheduled_queries\"\r\n      location=var.location\r\n      destination_dataset_id=google_bigquery_dataset.dataset.dataset_id\r\n      display_name = \"config_queries_${substr(each.value, 0, length(each.value) -4)}\"\r\n      schedule = \"every 4 hours \"\r\n     params = {\r\n        destination_table_name_template = substr(each.value,0,length(each.value) -4)\r\n        write_disposition = \"WRITE_APPEND\"\r\n        query = file(\"${path.module}/scheduled_queries/${each.value}\")\r\n      }\r\n    }\r\n\r\nI am getting below error:\r\n\r\n\u2502 Error: Error creating Config: googleapi: Error 404: Requested entity was not found.\r\n\r\nThe same code is working fine if running sql files individually.\r\nAny suggestions\r\n\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "258091272", "PostHistoryTypeId": "1", "PostId": "69988728", "RevisionGUID": "6a40759c-7454-4f47-a112-bed914d45171", "CreationDate": "2021-11-16T11:56:22.937", "UserId": "7639681", "Text": "Bigquery Terraform Schedule queries deployment error", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "258091273", "PostHistoryTypeId": "3", "PostId": "69988728", "RevisionGUID": "6a40759c-7454-4f47-a112-bed914d45171", "CreationDate": "2021-11-16T11:56:22.937", "UserId": "7639681", "Text": "<google-bigquery><terraform><terraform-provider-gcp>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}], "answers": [{"Id": "70096632", "PostTypeId": "2", "ParentId": "69988728", "CreationDate": "2021-11-24T13:05:24.010", "Score": "2", "Body": "<p>I will leave this community answer for visibility. Please feel free to update if it not reflect the actual answer. This is the original response found in the <a href=\"https://github.com/googleapis/python-bigquery-datatransfer/issues/21\" rel=\"nofollow noreferrer\">link</a> provided with the solution by <a href=\"https://github.com/paslandau\" rel=\"nofollow noreferrer\">paslandau</a> .</p>\n<p>As I've spent the better part of the day to make this work, here is the quick guide of what worked for me:</p>\n<ol>\n<li>Regards,As I've spent the better part of the day to make this work, here is the quick guide of what worked for me:</li>\n</ol>\n<p>Create a service account in Project-A at</p>\n<p><code>https://console.cloud.google.com/iam-admin/serviceaccounts?orgonly=true&amp;project=Project-A</code></p>\n<p>If you already have a service account, you can find its corresponding project id in its json keyfile</p>\n<pre><code>{\n  &quot;type&quot;: &quot;service_account&quot;,\n  &quot;project_id&quot;: &quot;Project-A&quot;,\n...\n</code></pre>\n<ol start=\"2\">\n<li><p>Activate the Big Query Transfer API for the project you want to create the transfer job at ( Project-B) via</p>\n<p><a href=\"https://console.cloud.google.com/marketplace/product/google/bigquerydatatransfer.googleapis.com?project=\" rel=\"nofollow noreferrer\">https://console.cloud.google.com/marketplace/product/google/bigquerydatatransfer.googleapis.com?project=</a></p>\n</li>\n<li><p>Add the service account from step 1 to Project-B with Role &quot;Big Query Admin&quot; via <a href=\"https://console.cloud.google.com/iam-admin/iam?project=Project-B\" rel=\"nofollow noreferrer\">https://console.cloud.google.com/iam-admin/iam?project=Project-B</a></p>\n</li>\n<li><p>Try to create a transfer job via API (i.e. &quot;from your code&quot;). This will trigger the error mentioned in the titel - which will also include the auto-generated transfer service service account of Project-B, e.g.</p>\n</li>\n</ol>\n<pre><code>400 P4 service account needs iam.serviceAccounts.getAccessToken permission. Running the following command may resolve this error: gcloud projects  add-iam-policy-binding &lt;PROJECT_ID&gt; --member='serviceAccount:service-&lt;PROJECT_NUMBER&gt;@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com'\n</code></pre>\n<p>Copy the name of the tranfer service account, e.g.</p>\n<ol start=\"5\">\n<li><p>Add the transfer service account to Project-A with the role &quot;Service Account Token Creator&quot; via</p>\n<p><a href=\"https://console.cloud.google.com/iam-admin/iam?project=Project-A\" rel=\"nofollow noreferrer\">https://console.cloud.google.com/iam-admin/iam?project=Project-A</a></p>\n</li>\n<li><p>Re-run the creation of the transfer service job as in step 4. It should now succeed.</p>\n</li>\n</ol>\n", "OwnerUserId": "14551567", "LastActivityDate": "2021-11-24T13:05:24.010", "CommentCount": "0", "CommunityOwnedDate": "2021-11-24T13:05:24.010", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "258668174", "PostHistoryTypeId": "2", "PostId": "70096632", "RevisionGUID": "5fd98ef0-4c69-4ccb-98b8-049f1029e033", "CreationDate": "2021-11-24T13:05:24.010", "UserId": "14551567", "Text": "I will leave this community answer for visibility. Please feel free to update if it not reflect the actual answer. This is the original response found in the [link](https://github.com/googleapis/python-bigquery-datatransfer/issues/21) provided with the solution by [paslandau](https://github.com/paslandau) . \r\n\r\nAs I've spent the better part of the day to make this work, here is the quick guide of what worked for me:\r\n\r\n1. Regards,As I've spent the better part of the day to make this work, here is the quick guide of what worked for me:\r\n\r\nCreate a service account in Project-A at \r\n\r\n`https://console.cloud.google.com/iam-admin/serviceaccounts?orgonly=true&project=Project-A`\r\n\r\nIf you already have a service account, you can find its corresponding project id in its json keyfile\r\n\r\n    {\r\n      \"type\": \"service_account\",\r\n      \"project_id\": \"Project-A\",\r\n    ...\r\n\r\n\r\n2. Activate the Big Query Transfer API for the project you want to create the transfer job at ( Project-B) via \r\n\r\n    https://console.cloud.google.com/marketplace/product/google/bigquerydatatransfer.googleapis.com?project=\r\n\r\n3. Add the service account from step 1 to Project-B with Role \"Big Query Admin\" via https://console.cloud.google.com/iam-admin/iam?project=Project-B\r\n\r\n4. Try to create a transfer job via API (i.e. \"from your code\"). This will trigger the error mentioned in the titel - which will also include the auto-generated transfer service service account of Project-B, e.g.\r\n```\r\n400 P4 service account needs iam.serviceAccounts.getAccessToken permission. Running the following command may resolve this error: gcloud projects  add-iam-policy-binding <PROJECT_ID> --member='serviceAccount:service-<PROJECT_NUMBER>@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com'\r\n```\r\n\r\nCopy the name of the tranfer service account, e.g. \r\n\r\n\r\n5. Add the transfer service account to Project-A with the role \"Service Account Token Creator\" via \r\n\r\n    https://console.cloud.google.com/iam-admin/iam?project=Project-A\r\n\r\n6. Re-run the creation of the transfer service job as in step 4. It should now succeed.\r\n\r\n\r\n\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Running the following command may resolve this error: gcloud projects add-iam-policy-binding --member='serviceAccount:service-@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com' ``` ", "keywords": ["policy"]}]}, {"Id": "258668176", "PostHistoryTypeId": "16", "PostId": "70096632", "RevisionGUID": "7ca616c1-ee37-4085-8c79-b083f200b941", "CreationDate": "2021-11-24T13:05:24.010", "UserId": "14551567", "filtered-sentences": []}], "filtered-sentences": []}, {"Id": "75478439", "PostTypeId": "2", "ParentId": "69988728", "CreationDate": "2023-02-16T22:14:19.550", "Score": "2", "Body": "<p>I had the same error, but while creating a single scheduled query. It turned out, the problem was me taking the liberty of changing <strong>data_source_id</strong> value into some custom string. After reverting to &quot;scheduled_query&quot; string, as it is provided in available examples, it worked. I believe the solution provided above deals with a different problem (IAM), which can be somewhat simplified by using explicit service account with BigQuery Admin role applied (<strong>service_account_name</strong> TF argument), after enabling bigquerydatatransfer API.</p>\n", "OwnerUserId": "20992238", "LastEditorUserId": "20992238", "LastEditDate": "2023-02-16T22:17:39.400", "LastActivityDate": "2023-02-16T22:17:39.400", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "288217194", "PostHistoryTypeId": "2", "PostId": "75478439", "RevisionGUID": "4c16fb88-8586-4487-ac5b-2e47e2be6f62", "CreationDate": "2023-02-16T22:14:19.550", "UserId": "20992238", "Text": "I had the same error, but while creating a single scheduled query. It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. After reverting to \"scheduled_query\" string, as it is provided in available examples, it worked. I believe the solution provided above deals with a different problem (IAM), which can be somewhat simplified by using explicit service account with BigQuery Admin role applied (**service_account_name** TF argument).", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. ", "keywords": ["change"]}]}, {"Id": "288217270", "PostHistoryTypeId": "5", "PostId": "75478439", "RevisionGUID": "14663126-b2e2-4215-9189-5b2ba55964f9", "CreationDate": "2023-02-16T22:16:07.897", "UserId": "20992238", "Comment": "added 70 characters in body", "Text": "I had the same error, but while creating a single scheduled query. It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. After reverting to \"scheduled_query\" string, as it is provided in available examples, it worked. I believe the solution provided above deals with a different problem (IAM), which can be somewhat simplified by using explicit service account with BigQuery Admin role applied (**service_account_name** TF argument).\r\n\r\nQuick edit: bigquerydatatransfer API was already enabled manually.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. ", "keywords": ["change"]}]}, {"Id": "288217308", "PostHistoryTypeId": "5", "PostId": "75478439", "RevisionGUID": "c88cb4cc-813e-4aa6-b8ec-e5e585af8a0d", "CreationDate": "2023-02-16T22:17:39.400", "UserId": "20992238", "Comment": "deleted 29 characters in body", "Text": "I had the same error, but while creating a single scheduled query. It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. After reverting to \"scheduled_query\" string, as it is provided in available examples, it worked. I believe the solution provided above deals with a different problem (IAM), which can be somewhat simplified by using explicit service account with BigQuery Admin role applied (**service_account_name** TF argument), after enabling bigquerydatatransfer API.", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "It turned out, the problem was me taking the liberty of changing **data_source_id** value into some custom string. ", "keywords": ["change"]}]}], "filtered-sentences": [{"source": "Body", "text": "It turned out, the problem was me taking the liberty of changing data_source_id value into some custom string. ", "keywords": ["change"]}]}], "contains-topic": true, "filtered-sentences": []}