{"Id": "76343133", "PostTypeId": "1", "AcceptedAnswerId": "76416544", "CreationDate": "2023-05-26T17:35:09.090", "Score": "0", "ViewCount": "69", "Body": "<p>I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern:</p>\n<pre><code>&lt;year&gt;/&lt;month&gt;/&lt;day&gt;/&lt;hour&gt;/&lt;id&gt;/&lt;start_minute&gt;:&lt;start_second&gt;-&lt;end_minute&gt;:&lt;end_second&gt;.jsonl\n</code></pre>\n<p>So for example:</p>\n<pre><code>2023/05/19/21/A9887e4d2f6fc1acb01/15:54-16:04.jsonl\n</code></pre>\n<p>I would like to load these to BigQuery into an hourly partitioned table.</p>\n<p>I'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the <code>data_path_template</code> so that it would only glob the previous hour's files.</p>\n<p>Or maybe it doesn't matter if it's using last modification times (as explained <a href=\"https://cloud.google.com/bigquery/docs/cloud-storage-transfer#limitations\" rel=\"nofollow noreferrer\">here</a>, relevant paragraph pasted below)?</p>\n<blockquote>\n<p>Transfers from Cloud Storage set the Write preference parameter to APPEND by default. In this mode, an unmodified file can only be loaded into BigQuery once. If the file's last modification time property is updated, then the file will be reloaded.</p>\n</blockquote>\n<p>I'm a bit worried about the extra scans as the bucket grows, but did I understand right that <a href=\"https://cloud.google.com/bigquery/pricing#bqdts\" rel=\"nofollow noreferrer\">BigQuery Data Transfer Service is free from Cloud Storage</a> anyway? Or would I still need to pay increasing <a href=\"https://cloud.google.com/storage/pricing#process-pricing\" rel=\"nofollow noreferrer\">Operation charges</a> for these scans?</p>\n<pre><code>resource &quot;google_bigquery_data_transfer_config&quot; &quot;query_config&quot; {\n  display_name           = &quot;sensor-data-ingestion&quot;\n  location               = var.region\n  data_source_id         = &quot;google_cloud_storage&quot;\n  schedule               = &quot;every 1 hours from 00:10 to 23:10&quot;\n  destination_dataset_id = google_bigquery_dataset.main.dataset_id\n  params = {\n    data_path_template              = &quot;gs://${google_storage_bucket.sensor-data.name}/*.jsonl&quot;\n    destination_table_name_template = &quot;sensor_data_partitioned&quot;\n    file_format                     = &quot;JSON&quot;\n    write_disposition               = &quot;WRITE_APPEND&quot;\n  }\n}\n</code></pre>\n", "OwnerUserId": "21217", "LastEditorUserId": "21217", "LastEditDate": "2023-06-01T11:29:50.933", "LastActivityDate": "2023-06-06T16:13:15.593", "Title": "Is it possible to have a dynamic `data_path_template` in Cloud Storage BigQuery Data Transfer Service?", "Tags": "<google-cloud-platform><google-bigquery><terraform><google-cloud-storage>", "AnswerCount": "1", "CommentCount": "2", "ContentLicense": "CC BY-SA 4.0", "comments": [{"Id": "134651007", "PostId": "76343133", "Score": "0", "Text": "Yes, you are correct that the transfer are free of charge as stated at the table, But separate charges may apply to BQ and Google Cloud Storage.", "CreationDate": "2023-05-29T21:16:31.140", "UserId": "19378826", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Yes, you are correct that the transfer are free of charge as stated at the table, But separate charges may apply to BQ and Google Cloud Storage.", "keywords": ["storage"]}]}, {"Id": "134669472", "PostId": "76343133", "Score": "0", "Text": "Right, and while using the file modification filter BQ charges should be fine as each data file will be ingested once, but Cloud Storage charges can apply for the scans.", "CreationDate": "2023-05-31T07:22:26.650", "UserId": "21217", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Right, and while using the file modification filter BQ charges should be fine as each data file will be ingested once, but Cloud Storage charges can apply for the scans.", "keywords": ["storage"]}]}], "history": [{"Id": "293356578", "PostHistoryTypeId": "2", "PostId": "76343133", "RevisionGUID": "7b942b65-f45b-4f81-ae88-4dd00d3ab441", "CreationDate": "2023-05-26T17:35:09.090", "UserId": "21217", "Text": "I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern:\r\n\r\n```\r\n<year>/<month>/<day>/<hour>/<id>/<start_minute>:<start_second>-<end_minute>:<end_second>.jsonl\r\n```\r\n\r\nSo for example:\r\n\r\n```\r\n2023/05/19/21/A9887e4d2f6fc1acb01/15:54-16:04.jsonl\r\n```\r\n\r\nI would like to load these to BigQuery into an hourly partitioned table.\r\n\r\nI'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the `data_path_template` so that it would only glob the previous hour's files.\r\n\r\nOr maybe it doesn't matter if it's using last modification times (as explained [here](https://cloud.google.com/bigquery/docs/cloud-storage-transfer#limitations), relevant paragraph pasted below)?\r\n\r\n> Transfers from Cloud Storage set the Write preference parameter to APPEND by default. In this mode, an unmodified file can only be loaded into BigQuery once. If the file's last modification time property is updated, then the file will be reloaded.\r\n\r\nI'm a bit worried about the extra scans as the bucket grows, but did I understand right that [BigQuery Data Transfer Service is free from Cloud Storage](https://cloud.google.com/bigquery/pricing#bqdts) anyway?\r\n\r\n```terraform\r\nresource \"google_bigquery_data_transfer_config\" \"query_config\" {\r\n  display_name           = \"sensor-data-ingestion\"\r\n  location               = var.region\r\n  data_source_id         = \"google_cloud_storage\"\r\n  schedule               = \"every 1 hours from 00:10 to 23:10\"\r\n  destination_dataset_id = google_bigquery_dataset.main.dataset_id\r\n  params = {\r\n    data_path_template              = \"gs://${google_storage_bucket.sensor-data.name}/*.jsonl\"\r\n    destination_table_name_template = \"sensor_data_partitioned\"\r\n    file_format                     = \"JSON\"\r\n    write_disposition               = \"WRITE_APPEND\"\r\n  }\r\n}\r\n```\r\n", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern: ``` /////:-:.jsonl ``` ", "keywords": ["storage"]}, {"source": "Text", "text": "I'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the `data_path_template` so that it would only glob the previous hour's files. ", "keywords": ["storage"]}, {"source": "Text", "text": "Or maybe it doesn't matter if it's using last modification times (as explained [here](https://cloud.google.com/bigquery/docs/cloud-storage-transfer#limitations), relevant paragraph pasted below)? ", "keywords": ["storage"]}, {"source": "Text", "text": "> Transfers from Cloud Storage set the Write preference parameter to APPEND by default. ", "keywords": ["storage"]}, {"source": "Text", "text": "In this mode, an unmodified file can only be loaded into BigQuery once. ", "keywords": ["billing mode"]}, {"source": "Text", "text": "I'm a bit worried about the extra scans as the bucket grows, but did I understand right that [BigQuery Data Transfer Service is free from Cloud Storage](https://cloud.google.com/bigquery/pricing#bqdts) anyway? ```terraform resource \"google_bigquery_data_transfer_config\" \"query_config\" { display_name = \"sensor-data-ingestion\" location = var.region data_source_id = \"google_cloud_storage\" schedule = \"every 1 hours from 00:10 to 23:10\" destination_dataset_id = google_bigquery_dataset.main.dataset_id params = { data_path_template = \"gs://${google_storage_bucket.sensor-data.name}/*.jsonl\" destination_table_name_template = \"sensor_data_partitioned\" file_format = \"JSON\" write_disposition = \"WRITE_APPEND\" ", "keywords": ["storage"]}]}, {"Id": "293356580", "PostHistoryTypeId": "1", "PostId": "76343133", "RevisionGUID": "7b942b65-f45b-4f81-ae88-4dd00d3ab441", "CreationDate": "2023-05-26T17:35:09.090", "UserId": "21217", "Text": "Is it possible to have a dynamic `data_path_template` in Cloud Storage BigQuery Data Transfer Service?", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Is it possible to have a dynamic `data_path_template` in Cloud Storage BigQuery Data Transfer Service?", "keywords": ["storage"]}]}, {"Id": "293356581", "PostHistoryTypeId": "3", "PostId": "76343133", "RevisionGUID": "7b942b65-f45b-4f81-ae88-4dd00d3ab441", "CreationDate": "2023-05-26T17:35:09.090", "UserId": "21217", "Text": "<google-cloud-platform><google-bigquery><terraform><google-cloud-storage>", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": []}, {"Id": "293596181", "PostHistoryTypeId": "5", "PostId": "76343133", "RevisionGUID": "c0ddb3af-d8b2-44ba-88a0-7fc2d460e252", "CreationDate": "2023-06-01T11:29:50.933", "UserId": "21217", "Comment": "added 147 characters in body", "Text": "I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern:\r\n\r\n```\r\n<year>/<month>/<day>/<hour>/<id>/<start_minute>:<start_second>-<end_minute>:<end_second>.jsonl\r\n```\r\n\r\nSo for example:\r\n\r\n```\r\n2023/05/19/21/A9887e4d2f6fc1acb01/15:54-16:04.jsonl\r\n```\r\n\r\nI would like to load these to BigQuery into an hourly partitioned table.\r\n\r\nI'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the `data_path_template` so that it would only glob the previous hour's files.\r\n\r\nOr maybe it doesn't matter if it's using last modification times (as explained [here](https://cloud.google.com/bigquery/docs/cloud-storage-transfer#limitations), relevant paragraph pasted below)?\r\n\r\n> Transfers from Cloud Storage set the Write preference parameter to APPEND by default. In this mode, an unmodified file can only be loaded into BigQuery once. If the file's last modification time property is updated, then the file will be reloaded.\r\n\r\nI'm a bit worried about the extra scans as the bucket grows, but did I understand right that [BigQuery Data Transfer Service is free from Cloud Storage](https://cloud.google.com/bigquery/pricing#bqdts) anyway? Or would I still need to pay increasing [Operation charges][1] for these scans?\r\n\r\n```terraform\r\nresource \"google_bigquery_data_transfer_config\" \"query_config\" {\r\n  display_name           = \"sensor-data-ingestion\"\r\n  location               = var.region\r\n  data_source_id         = \"google_cloud_storage\"\r\n  schedule               = \"every 1 hours from 00:10 to 23:10\"\r\n  destination_dataset_id = google_bigquery_dataset.main.dataset_id\r\n  params = {\r\n    data_path_template              = \"gs://${google_storage_bucket.sensor-data.name}/*.jsonl\"\r\n    destination_table_name_template = \"sensor_data_partitioned\"\r\n    file_format                     = \"JSON\"\r\n    write_disposition               = \"WRITE_APPEND\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n  [1]: https://cloud.google.com/storage/pricing#process-pricing", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern: ``` /////:-:.jsonl ``` ", "keywords": ["storage"]}, {"source": "Text", "text": "I'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the `data_path_template` so that it would only glob the previous hour's files. ", "keywords": ["storage"]}, {"source": "Text", "text": "Or maybe it doesn't matter if it's using last modification times (as explained [here](https://cloud.google.com/bigquery/docs/cloud-storage-transfer#limitations), relevant paragraph pasted below)? ", "keywords": ["storage"]}, {"source": "Text", "text": "> Transfers from Cloud Storage set the Write preference parameter to APPEND by default. ", "keywords": ["storage"]}, {"source": "Text", "text": "In this mode, an unmodified file can only be loaded into BigQuery once. ", "keywords": ["billing mode"]}, {"source": "Text", "text": "I'm a bit worried about the extra scans as the bucket grows, but did I understand right that [BigQuery Data Transfer Service is free from Cloud Storage](https://cloud.google.com/bigquery/pricing#bqdts) anyway? ", "keywords": ["storage"]}, {"source": "Text", "text": "Or would I still need to pay increasing [Operation charges][1] for these scans? ```terraform resource \"google_bigquery_data_transfer_config\" \"query_config\" { display_name = \"sensor-data-ingestion\" location = var.region data_source_id = \"google_cloud_storage\" schedule = \"every 1 hours from 00:10 to 23:10\" destination_dataset_id = google_bigquery_dataset.main.dataset_id params = { data_path_template = \"gs://${google_storage_bucket.sensor-data.name}/*.jsonl\" destination_table_name_template = \"sensor_data_partitioned\" file_format = \"JSON\" write_disposition = \"WRITE_APPEND\" ", "keywords": ["pay"]}, {"source": "Text", "text": "} ``` [1]: https://cloud.google.com/storage/pricing#process-pricing", "keywords": ["storage"]}]}], "answers": [{"Id": "76416544", "PostTypeId": "2", "ParentId": "76343133", "CreationDate": "2023-06-06T16:13:15.593", "Score": "0", "Body": "<p>Spent way too much time on this, but found out that the answer is yes!</p>\n<p>Apparently, there is a templating system for both the bucket URI and the table partition, documented <a href=\"https://cloud.google.com/bigquery/docs/gcs-transfer-parameters#load_a_snapshot_of_all_data_into_an_ingestion-time_partitioned_table\" rel=\"nofollow noreferrer\">here</a> and it looks like this for Cloud Storage (also available for S3 and Blob Storage):</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Data source</th>\n<th>Parameterized URI or data path</th>\n<th>Parameterized destination table name</th>\n<th>Evaluated URI or data path</th>\n<th>Evaluated destination table name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cloud Storage</td>\n<td>gs://bucket/events-{run_time|&quot;%Y%m%d&quot;}/*.csv</td>\n<td>mytable${run_time|&quot;%Y%m%d&quot;}</td>\n<td>gs://bucket/events-20180215/*.csv</td>\n<td>mytable$20180215</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Even better, it supports time and offsetting from transfer run time too! Documented <a href=\"https://cloud.google.com/bigquery/docs/s3-transfer-parameters#param-templating-syntax\" rel=\"nofollow noreferrer\">here</a>, see for example:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>run_time (UTC)</th>\n<th>Templated parameter</th>\n<th>Output destination table name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>mytable</td>\n<td>mytable</td>\n</tr>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>mytable_{run_time|&quot;%Y%m%d&quot;}</td>\n<td>mytable_20180215</td>\n</tr>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>mytable_{run_time+25h|&quot;%Y%m%d&quot;}</td>\n<td>mytable_20180216</td>\n</tr>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>mytable_{run_time-1h|&quot;%Y%m%d&quot;}</td>\n<td>mytable_20180214</td>\n</tr>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>mytable_{run_time+1.5h|&quot;%Y%m%d%H&quot;} or mytable_{run_time+90m|&quot;%Y%m%d%H&quot;}</td>\n<td>mytable_2018021501</td>\n</tr>\n<tr>\n<td>2018-02-15 00:00:00</td>\n<td>{run_time+97s|&quot;%Y%m%d&quot;}_mytable_{run_time+97s|&quot;%H%M%S&quot;}</td>\n<td>20180215_mytable_000137</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So, a working Terraform example would be (note that both &quot; and $ needs escaping):</p>\n<pre><code>resource &quot;google_bigquery_data_transfer_config&quot; &quot;sensor-data-ingestion&quot; {\n  depends_on             = [google_storage_bucket.sensor-data, google_project_iam_member.sensor-data-ingestion-token-creator, google_project_iam_member.sensor-data-ingestion-data-editor, google_bigquery_table.sensor-data-main-partitioned, google_service_account.sensor-data-ingestion-service-account]\n  display_name           = &quot;sensor-data-transfer&quot;\n  location               = var.region\n  data_source_id         = &quot;google_cloud_storage&quot;\n  schedule               = &quot;every 1 hours from 00:10 to 23:10&quot;\n  destination_dataset_id = google_bigquery_dataset.main.dataset_id\n  service_account_name   = google_service_account.sensor-data-ingestion-service-account.email\n  params = {\n    data_path_template              = &quot;gs://${google_storage_bucket.sensor-data.name}/{run_time-1h|\\&quot;%Y%m%d%H\\&quot;}*.jsonl&quot;\n    destination_table_name_template = &quot;${google_bigquery_table.sensor-data-main-partitioned.table_id}$${run_time-1h|\\&quot;%Y%m%d%H\\&quot;}&quot;\n    file_format                     = &quot;JSON&quot;\n    write_disposition               = &quot;APPEND&quot;\n  }\n}\n</code></pre>\n<p>I wasted a couple of days trying to make wildcard globbing work with paths using <code>/</code> but had to remove them and use <code>_</code> as separator, so eventually settled on file names like <code>2023060615_A9887e4d2f6fc1acb01_50_51-50_51.jsonl</code>.</p>\n", "OwnerUserId": "21217", "LastActivityDate": "2023-06-06T16:13:15.593", "CommentCount": "0", "ContentLicense": "CC BY-SA 4.0", "history": [{"Id": "293812723", "PostHistoryTypeId": "2", "PostId": "76416544", "RevisionGUID": "cb075db1-c357-4a2a-b8f6-dd741bf869d0", "CreationDate": "2023-06-06T16:13:15.593", "UserId": "21217", "Text": "Spent way too much time on this, but found out that the answer is yes!\r\n\r\nApparently, there is a templating system for both the bucket URI and the table partition, documented [here][1] and it looks like this for Cloud Storage (also available for S3 and Blob Storage):\r\n\r\n|Data source | Parameterized URI or data path | Parameterized destination table name | Evaluated URI or data path | Evaluated destination table name\r\n|---|---|---|---|---|\r\nCloud Storage | gs://bucket/events-{run_time\\|\"%Y%m%d\"}/*.csv | mytable${run_time\\|\"%Y%m%d\"} | gs://bucket/events-20180215/*.csv | mytable$20180215\r\n\r\nEven better, it supports time and offsetting from transfer run time too! Documented [here][2], see for example:\r\n\r\nrun_time (UTC) | Templated parameter | Output destination table name\r\n|---|---|---|\r\n2018-02-15 00:00:00 | mytable | mytable\r\n2018-02-15 00:00:00 | mytable_{run_time\\|\"%Y%m%d\"} | mytable_20180215\r\n2018-02-15 00:00:00 | mytable_{run_time+25h\\|\"%Y%m%d\"} | mytable_20180216\r\n2018-02-15 00:00:00 | mytable_{run_time-1h\\|\"%Y%m%d\"} | mytable_20180214\r\n2018-02-15 00:00:00 | mytable_{run_time+1.5h\\|\"%Y%m%d%H\"} or mytable_{run_time+90m\\|\"%Y%m%d%H\"} | mytable_2018021501\r\n2018-02-15 00:00:00 | {run_time+97s\\|\"%Y%m%d\"}\\_mytable\\_{run_time+97s\\|\"%H%M%S\"} | 20180215_mytable_000137\r\n\r\nSo, a working Terraform example would be (note that both \" and $ needs escaping):\r\n\r\n```terraform\r\nresource \"google_bigquery_data_transfer_config\" \"sensor-data-ingestion\" {\r\n  depends_on             = [google_storage_bucket.sensor-data, google_project_iam_member.sensor-data-ingestion-token-creator, google_project_iam_member.sensor-data-ingestion-data-editor, google_bigquery_table.sensor-data-main-partitioned, google_service_account.sensor-data-ingestion-service-account]\r\n  display_name           = \"sensor-data-transfer\"\r\n  location               = var.region\r\n  data_source_id         = \"google_cloud_storage\"\r\n  schedule               = \"every 1 hours from 00:10 to 23:10\"\r\n  destination_dataset_id = google_bigquery_dataset.main.dataset_id\r\n  service_account_name   = google_service_account.sensor-data-ingestion-service-account.email\r\n  params = {\r\n    data_path_template              = \"gs://${google_storage_bucket.sensor-data.name}/{run_time-1h|\\\"%Y%m%d%H\\\"}*.jsonl\"\r\n    destination_table_name_template = \"${google_bigquery_table.sensor-data-main-partitioned.table_id}$${run_time-1h|\\\"%Y%m%d%H\\\"}\"\r\n    file_format                     = \"JSON\"\r\n    write_disposition               = \"APPEND\"\r\n  }\r\n}\r\n```\r\n\r\nI wasted a couple of days trying to make wildcard globbing work with paths using `/` but had to remove them and use `_` as separator, so eventually settled on file names like `2023060615_A9887e4d2f6fc1acb01_50_51-50_51.jsonl`.\r\n\r\n  [1]: https://cloud.google.com/bigquery/docs/gcs-transfer-parameters#load_a_snapshot_of_all_data_into_an_ingestion-time_partitioned_table\r\n  [2]: https://cloud.google.com/bigquery/docs/s3-transfer-parameters#param-templating-syntax", "ContentLicense": "CC BY-SA 4.0", "filtered-sentences": [{"source": "Text", "text": "Apparently, there is a templating system for both the bucket URI and the table partition, documented [here][1] and it looks like this for Cloud Storage (also available for S3 and Blob Storage): |Data source | Parameterized URI or data path | Parameterized destination table name | Evaluated URI or data path | Evaluated destination table name |---|---|---|---|---| Cloud Storage | gs://bucket/events-{run_time\\|\"%Y%m%d\"}/*.csv | mytable${run_time\\|\"%Y%m%d\"} | gs://bucket/events-20180215/*.csv | mytable$20180215 Even better, it supports time and offsetting from transfer run time too! Documented [here][2], see for example: run_time (UTC) | Templated parameter | Output destination table name |---|---|---| 2018-02-15 00:00:00 | mytable | mytable 2018-02-15 00:00:00 | mytable_{run_time\\|\"%Y%m%d\"} | mytable_20180215 2018-02-15 00:00:00 | mytable_{run_time+25h\\|\"%Y%m%d\"} | mytable_20180216 2018-02-15 00:00:00 | mytable_{run_time-1h\\|\"%Y%m%d\"} | mytable_20180214 2018-02-15 00:00:00 | mytable_{run_time+1.5h\\|\"%Y%m%d%H\"} or mytable_{run_time+90m\\|\"%Y%m%d%H\"} | mytable_2018021501 2018-02-15 00:00:00 | {run_time+97s\\|\"%Y%m%d\"}\\_mytable\\_{run_time+97s\\|\"%H%M%S\"} | 20180215_mytable_000137 So, a working Terraform example would be (note that both \" and $ needs escaping): ```terraform resource \"google_bigquery_data_transfer_config\" \"sensor-data-ingestion\" { depends_on = [google_storage_bucket.sensor-data, google_project_iam_member.sensor-data-ingestion-token-creator, google_project_iam_member.sensor-data-ingestion-data-editor, google_bigquery_table.sensor-data-main-partitioned, google_service_account.sensor-data-ingestion-service-account] display_name = \"sensor-data-transfer\" location = var.region data_source_id = \"google_cloud_storage\" schedule = \"every 1 hours from 00:10 to 23:10\" destination_dataset_id = google_bigquery_dataset.main.dataset_id service_account_name = google_service_account.sensor-data-ingestion-service-account.email params = { data_path_template = \"gs://${google_storage_bucket.sensor-data.name}/{run_time-1h|\\\"%Y%m%d%H\\\"}*.jsonl\" destination_table_name_template = \"${google_bigquery_table.sensor-data-main-partitioned.table_id}$${run_time-1h|\\\"%Y%m%d%H\\\"}\" file_format = \"JSON\" write_disposition = \"APPEND\" } } ``` ", "keywords": ["storage"]}]}], "filtered-sentences": [{"source": "Body", "text": "Apparently, there is a templating system for both the bucket URI and the table partition, documented here and it looks like this for Cloud Storage (also available for S3 and Blob Storage): Data source Parameterized URI or data path Parameterized destination table name Evaluated URI or data path Evaluated destination table name Cloud Storage gs://bucket/events-{run_time|\"%Y%m%d\"}/*.csv mytable${run_time|\"%Y%m%d\"} gs://bucket/events-20180215/*.csv mytable$20180215 Even better, it supports time and offsetting from transfer run time too! ", "keywords": ["storage"]}]}], "contains-topic": true, "filtered-sentences": [{"source": "Title", "text": "Is it possible to have a dynamic `data_path_template` in Cloud Storage BigQuery Data Transfer Service?", "keywords": ["storage"]}, {"source": "Body", "text": "I have a Google Storage Bucket that will have timestamped JSONL files uploaded with the following path pattern: So for example: I would like to load these to BigQuery into an hourly partitioned table. ", "keywords": ["storage"]}, {"source": "Body", "text": "I'm trying to set up a scheduled Cloud Storage BigQuery Data Transfer Service for this purpose (using Terraform, see below), but a bit stumped how to specify the data_path_template so that it would only glob the previous hour's files. ", "keywords": ["storage"]}, {"source": "Body", "text": "Transfers from Cloud Storage set the Write preference parameter to APPEND by default. ", "keywords": ["storage"]}, {"source": "Body", "text": "In this mode, an unmodified file can only be loaded into BigQuery once. ", "keywords": ["billing mode"]}, {"source": "Body", "text": "I'm a bit worried about the extra scans as the bucket grows, but did I understand right that BigQuery Data Transfer Service is free from Cloud Storage anyway? ", "keywords": ["storage"]}, {"source": "Body", "text": "Or would I still need to pay increasing Operation charges for these scans?", "keywords": ["pay"]}]}